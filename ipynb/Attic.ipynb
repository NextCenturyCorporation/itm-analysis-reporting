{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "notice_to_control_time_df = pd.DataFrame({'notice_to_control_time': [565204, 66933, 180636, 170767, 266339, 255253, 244320, 82204, 93897, 363529]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "values_list = random.sample(notice_to_control_time_df.notice_to_control_time.tolist(), 10)\n",
    "print(f'''\n",
    "notice_to_control_time_df = pd.DataFrame({{'notice_to_control_time': {values_list}}})''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ".sort_values(list(columns_dict.values()), ascending=[False]*len(columns_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # print(previous_time, elapsed_time, session_uuid, time_group, patient_id, previous_time > elapsed_time)\n",
    "            if (previous_time > elapsed_time):\n",
    "                display(df1[mask_series].sort_values('elapsed_time').head(4).dropna(axis='columns', how='all').T)\n",
    "                display(df2.sample(min(4, df2.shape[0])).dropna(axis='columns', how='all').T)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print(f\"\"\"\n",
    "    groupby_columns = {df.columns.tolist()}\n",
    "    gb = df.groupby(groupby_columns)\n",
    "    rows_list = []\n",
    "    for ({', '.join(df.columns)}), df1 in gb:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Investigate the presence of trunk, neck, junctional or extremity info\n",
    "    for search_str in ['trunk', 'neck', 'junctional', 'extremity']:\n",
    "        srs = df2.applymap(lambda x: search_str in str(x).lower(), na_action='ignore').sum()\n",
    "        columns_list = srs[srs != 0].index.tolist()\n",
    "        if columns_list:\n",
    "            print(search_str, columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required_columns_list = [cn for cn in frvrs_logs_df.columns if cn.endswith('_required_procedure')]\n",
    "mask_series = False\n",
    "for cn in required_columns_list: mask_series |= (frvrs_logs_df[cn].isin(['tourniquet', 'woundpack']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn = lambda x: ('gaze' in str(x).lower())\n",
    "srs = frvrs_logs_df.applymap(fn, na_action='ignore').sum()\n",
    "columns_list = srs[srs != 0].index.tolist()\n",
    "print(columns_list)\n",
    "for cn in columns_list:\n",
    "    print(cn)\n",
    "    mask_series = frvrs_logs_df[cn].map(fn)\n",
    "    df = frvrs_logs_df[mask_series]\n",
    "    \n",
    "    # Display a sample of the data frame, dropping columns with all NaN values and transposing it\n",
    "    display(df.sample(min(4, df.shape[0])).dropna(axis='columns', how='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tick_locations = ax.get_xticks()\n",
    "# print(tick_locations)\n",
    "\n",
    "# tick_labels = ax.get_xticklabels()\n",
    "# print(tick_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Look at the new actions\n",
    "df = DataFrame(gaze_rows_list)\n",
    "mask_series = (df.action_type == 'PLAYER_GAZE')\n",
    "df[mask_series].sample(min(4, df[mask_series].shape[0])).dropna(axis='columns', how='all').T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Write a note to get the schema\n",
    "old_actions_list = [\n",
    "    'SESSION_END', 'SESSION_START', 'BAG_ACCESS', 'BAG_CLOSED', 'INJURY_RECORD', 'INJURY_TREATED', 'PATIENT_DEMOTED',\n",
    "    'PATIENT_ENGAGED', 'PATIENT_RECORD', 'PULSE_TAKEN', 'S_A_L_T_WALKED', 'S_A_L_T_WALK_IF_CAN', 'S_A_L_T_WAVED',\n",
    "    'S_A_L_T_WAVE_IF_CAN', 'TAG_APPLIED', 'TAG_DISCARDED', 'TAG_SELECTED', 'TELEPORT', 'TOOL_APPLIED', 'TOOL_DISCARDED',\n",
    "    'TOOL_HOVER', 'TOOL_SELECTED', 'VOICE_CAPTURE', 'VOICE_COMMAND'\n",
    "]\n",
    "missing_actions_list = list(set(frvrs_logs_df.action_type.unique()) - set(old_actions_list))\n",
    "print(missing_actions_list)\n",
    "print(f'@Jeremy Patterson, do you have an updated MCIVR Metrics Types document? I need {nu.conjunctify_nouns(missing_actions_list)}.')\n",
    "for action in missing_actions_list:\n",
    "    print(f'''        elif (action_type == '{action}'): # {''.join([x.title() for x in action.split('_')])}\n",
    "            frvrs_logs_df.loc[row_index, '{action.lower()}_location'] = row_series[4] # Location\n",
    "            frvrs_logs_df.loc[row_index, '{action.lower()}_patient_id'] = row_series[5] # patientId\n",
    "            frvrs_logs_df.loc[row_index, '{action.lower()}_distance'] = row_series[7] # distance\n",
    "            frvrs_logs_df.loc[row_index, '{action.lower()}_rotation'] = row_series[8] # rotation''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "text_editor_path = r'C:\\Program Files\\Notepad++\\notepad++.exe'\n",
    "!\"{text_editor_path}\" \"{os.path.abspath(file_path)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints/*\n",
      ".ipynb_checkpoints*\n",
      ".ipynb_checkpoints/\n",
      "**/.ipynb_checkpoints/\n",
      ".ipynb_checkpoints\n",
      "*.ipynb_checkpoints\n",
      "/.ipynb_checkpoints\n",
      "*/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n'.join(sorted('''**/.ipynb_checkpoints/\n",
    "*.ipynb_checkpoints\n",
    "*/.ipynb_checkpoints\n",
    ".ipynb_checkpoints\n",
    ".ipynb_checkpoints*\n",
    ".ipynb_checkpoints/\n",
    ".ipynb_checkpoints/*\n",
    "/.ipynb_checkpoints'''.split('\\n'), key=lambda x: x[::-1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT Stuff (Python 3.10.10)",
   "language": "python",
   "name": "gs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
