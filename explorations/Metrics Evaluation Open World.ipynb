{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83367fa2-cc2c-404d-bee3-cc7518f3a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Access modules in the py folder\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936fa00f-f4ac-42c0-ba0b-dfa9e03bde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get needed libraries\n",
    "from FRVRS import fu, nu\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from pandas import DataFrame, Series, concat, notnull\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f650d50c-233f-4c3c-8d3f-656d5741ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/metrics_evaluation_open_world_df.pkl.\n",
      "Attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/metrics_evaluation_open_world_file_stats_df.pkl.\n",
      "Attempting to load /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/metrics_evaluation_open_world_scene_stats_df.pkl.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data frames\n",
    "data_frames_list = nu.load_data_frames(\n",
    "    metrics_evaluation_open_world_df='', metrics_evaluation_open_world_file_stats_df='', metrics_evaluation_open_world_scene_stats_df=''\n",
    ")\n",
    "logs_df = data_frames_list['metrics_evaluation_open_world_df']\n",
    "file_stats_df = data_frames_list['metrics_evaluation_open_world_file_stats_df']\n",
    "scene_stats_df = data_frames_list['metrics_evaluation_open_world_scene_stats_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50d7ee-2e7c-41a1-a378-aa9b6cefc332",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset Built for Metrics Evaluation Open World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852efff6-2c26-4e85-bcff-9127fbc1e09b",
   "metadata": {},
   "source": [
    "\n",
    "Conduct some exploratory analysis of the open world segments for the ITM scenarios from the Metrics Evaluation.\n",
    "For context, results of these analyses is a goal for the 4/30 results meeting (stretch) or the PI meeting (more likely).\n",
    "\n",
    "<h2>Which factors contribute to the variance in these outcomes?</h2>\n",
    "Conceptually, I want an exploratory factor analysis using these IVs and DVs. But I suspect we don’t have enough data for that so as close as we can get to that, let’s get creative.\n",
    "My thought was to keep the environments separate because each participant did 2 of the environments so if we use 1 to explore, we can use the other to confirm. But again, I recognize we do not have power to do these properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf708d9-fbfa-4f73-a3b3-f577adad16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80364, 142)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add file and scene columns to the 11-patient scenes\n",
    "participant_columns = [\n",
    "    'ST_Del_Text', 'AD_AttribGrp_Sim', 'Trust', 'ST_ConfFC_Text', 'Delegation', 'YrsMilExp', 'AD_Del_Text', 'MedExp', 'AD_AttribGrp_Text',\n",
    "    'ST_ConfFC_Omni_Text', 'PropTrust', 'ST_KDMA_Text', 'AD_KDMA_Text', 'ST_AttribGrp_Text', 'ST_Del_Omni_Text', 'MilitaryExp', 'AD_ConfFC_Omni_Text',\n",
    "    'MedRole', 'ST_AttribGrp_Sim', 'AD_ConfFC_Text', 'ST_KDMA_Sim', 'AD_Del_Omni_Text', 'AD_KDMA_Sim'\n",
    "]\n",
    "needed_columns = [\n",
    "    'scene_type', 'is_scene_aborted', 'is_a_one_triage_file', 'responder_category', 'responder_type', 'configData_scene',\n",
    "    'configData_scenarioData_name', 'configData_scenarioData_description', 'encounter_layout', 'participantId', 'ParticipantID', 'Sim1', 'Sim2'\n",
    "] + participant_columns\n",
    "elevens_df = fu.get_elevens_data_frame(\n",
    "    logs_df, file_stats_df, scene_stats_df, needed_columns=needed_columns\n",
    ")\n",
    "print(elevens_df.shape) # (80364, 138)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bde61-dd6d-46f0-a9dd-eaba221a3cff",
   "metadata": {},
   "source": [
    "\n",
    "## There should be a total of 23 participants labeled 2024201 to 2024223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902de5b-f0e0-425f-b2eb-8499f7dd4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series1 = (file_stats_df.ParticipantID == participant_id)\n",
    "mask_series2 = file_stats_df.participantId.map(lambda x: str(x) == str(participant_id))\n",
    "print(\n",
    "    participant_id,\n",
    "    file_stats_df[mask_series1].shape[0],\n",
    "    file_stats_df[mask_series2].shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8a9c570-d223-4d3a-acb5-cbaedd821ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024201</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024203</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024204</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024205</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024207</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024208</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024212</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024214</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024215</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024216</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024217</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024219</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024221</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024223</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a table of the count of JSON files that have the participant ID suffixed to their file name\n",
    "from IPython.display import HTML\n",
    "\n",
    "logs_path = '../data/logs/Metrics Evaluation Open World'\n",
    "rows_list = []\n",
    "for participant_id in range(2_024_201, 2_024_223+1):\n",
    "    row_dict = {'participant_id': participant_id}\n",
    "    file_count = 0\n",
    "    for sub_directory, directories_list, files_list in os.walk(logs_path):\n",
    "        for file_name in files_list:\n",
    "            if file_name.endswith('.json') and ('_' in file_name):\n",
    "                if (file_name.split('_')[1].split('.')[0] == str(participant_id)):\n",
    "                    file_count += 1\n",
    "    row_dict['file_count'] = file_count\n",
    "    # print(participant_id, file_count, 'files with the participant ID in the file name')\n",
    "    rows_list.append(row_dict)\n",
    "print()\n",
    "HTML(DataFrame(rows_list).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c13724b0-ca83-4a4a-9d78-0a9fb5f0b3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>folder0</th>\n",
       "      <th>folder1</th>\n",
       "      <th>folder2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024201</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201</td>\n",
       "      <td>1241b7f0-5262-4dc2-9102-2425b9b42864_2024201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024201</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201</td>\n",
       "      <td>23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024201</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>bccb0095-5efd-4c5c-ad58-8b8624f9ab56_2024201</td>\n",
       "      <td>bccb0095-5efd-4c5c-ad58-8b8624f9ab56_2024201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024202</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>220b609b-0e35-454e-9afd-c84cbfa3e3ad_2024202</td>\n",
       "      <td>220b609b-0e35-454e-9afd-c84cbfa3e3ad_2024202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024202</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>922ad146-241a-4ea6-8ff1-413d7e0d16ec_2024202</td>\n",
       "      <td>922ad146-241a-4ea6-8ff1-413d7e0d16ec_2024202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024203</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>4bc46c8c-66e7-463d-b3a1-2a8303af4fd1_2024203</td>\n",
       "      <td>4bc46c8c-66e7-463d-b3a1-2a8303af4fd1_2024203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024203</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>acf74a81-a534-44c7-9cb1-67ec381b5ee0_2024203</td>\n",
       "      <td>acf74a81-a534-44c7-9cb1-67ec381b5ee0_2024203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024204</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>80f79d45-22fd-479d-b6e2-c62b5778e073_2024204</td>\n",
       "      <td>80f79d45-22fd-479d-b6e2-c62b5778e073_2024204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024204</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>cbbf410f-4657-428e-9616-8a777cc4704d_2024204</td>\n",
       "      <td>cbbf410f-4657-428e-9616-8a777cc4704d_2024204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024205</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>8cc40587-ff7d-4ebd-98ee-6441baedf7e0_2024205</td>\n",
       "      <td>8cc40587-ff7d-4ebd-98ee-6441baedf7e0_2024205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024205</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>df2fcf88-874b-4cf9-9707-3fa0b30c348f_2024205</td>\n",
       "      <td>df2fcf88-874b-4cf9-9707-3fa0b30c348f_2024205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024206</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>499179ba-3138-4bae-918e-ffc7fb943760_2024206</td>\n",
       "      <td>499179ba-3138-4bae-918e-ffc7fb943760_2024206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024207</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>04f80090-9e61-431d-8473-dccb75fed04d_2024207</td>\n",
       "      <td>04f80090-9e61-431d-8473-dccb75fed04d_2024207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024207</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>66c2b794-b3b7-4d8a-a9fa-0e3338931eab_2024207</td>\n",
       "      <td>66c2b794-b3b7-4d8a-a9fa-0e3338931eab_2024207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024207</td>\n",
       "      <td>ITM 3.14.2024 405F</td>\n",
       "      <td>8839e3b8-be5e-4878-8aaf-26c656ae2270_2024207</td>\n",
       "      <td>8839e3b8-be5e-4878-8aaf-26c656ae2270_2024207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024208</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>0bc62b93-ac4d-40ef-bdfe-1d7badb24f70_2024208</td>\n",
       "      <td>0bc62b93-ac4d-40ef-bdfe-1d7badb24f70_2024208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024208</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>67dc0230-511d-41ac-ae9b-850900ab9e6a_2024208</td>\n",
       "      <td>67dc0230-511d-41ac-ae9b-850900ab9e6a_2024208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024208</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>bceedc17-7d27-42bd-aa14-d6a98f16c6a5_2024208</td>\n",
       "      <td>bceedc17-7d27-42bd-aa14-d6a98f16c6a5_2024208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024209</td>\n",
       "      <td>ITM 3.14.2024 405E</td>\n",
       "      <td>b5989edc-8348-4b84-b649-87fc4f1cca53_2024209</td>\n",
       "      <td>b5989edc-8348-4b84-b649-87fc4f1cca53_2024209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024212</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>2e8f6555-a7fa-4b54-8132-c030d697b4ad_2024212</td>\n",
       "      <td>2e8f6555-a7fa-4b54-8132-c030d697b4ad_2024212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024212</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>5a909dc4-0f86-430d-a400-e37139cba69f_2024212</td>\n",
       "      <td>5a909dc4-0f86-430d-a400-e37139cba69f_2024212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024212</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>a1bcac92-c01f-485a-9ce0-8198739b2c27_2024212</td>\n",
       "      <td>a1bcac92-c01f-485a-9ce0-8198739b2c27_2024212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024214</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>0c0cc880-b3fb-488d-a468-0e67c17ca176_2024214</td>\n",
       "      <td>0c0cc880-b3fb-488d-a468-0e67c17ca176_2024214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024214</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492_2024214</td>\n",
       "      <td>0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492_2024214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024214</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>12988238-24b6-4ac2-9f33-398321e82ae0_2024214</td>\n",
       "      <td>12988238-24b6-4ac2-9f33-398321e82ae0_2024214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024214</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>d2aaf0ef-a32f-4255-b3f5-56df927ae0b4_2024214</td>\n",
       "      <td>d2aaf0ef-a32f-4255-b3f5-56df927ae0b4_2024214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024215</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>3cf14e31-f416-4c78-8a69-91bf0c685448_2024215</td>\n",
       "      <td>3cf14e31-f416-4c78-8a69-91bf0c685448_2024215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024215</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>56d09dbd-9f4b-41e8-bfb3-143832e61d94_2024215</td>\n",
       "      <td>56d09dbd-9f4b-41e8-bfb3-143832e61d94_2024215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024215</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>7e23cc31-422a-42e1-acb5-964c661750f4_2024215</td>\n",
       "      <td>7e23cc31-422a-42e1-acb5-964c661750f4_2024215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024215</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>a7ce6f7b-6466-4281-9496-92b640d9d04b_2024215</td>\n",
       "      <td>a7ce6f7b-6466-4281-9496-92b640d9d04b_2024215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2024216</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>c6d3a90f-68c0-4948-bd96-537e80973605_2024216</td>\n",
       "      <td>c6d3a90f-68c0-4948-bd96-537e80973605_2024216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2024216</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>d13091cc-98e4-4aba-8d02-7eca8bd1a30c_2024216</td>\n",
       "      <td>d13091cc-98e4-4aba-8d02-7eca8bd1a30c_2024216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2024217</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>1995e7ef-ef02-4fc1-b1ab-f137dbf69d48_2024217</td>\n",
       "      <td>1995e7ef-ef02-4fc1-b1ab-f137dbf69d48_2024217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024217</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>6db9446c-2cd4-41b4-be8d-be5ccbbc6e05_2024217</td>\n",
       "      <td>6db9446c-2cd4-41b4-be8d-be5ccbbc6e05_2024217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2024217</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>c23df057-ab53-4276-b791-ebe38c431df7_2024217</td>\n",
       "      <td>c23df057-ab53-4276-b791-ebe38c431df7_2024217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024218</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>37a554ee-fc49-4730-819c-2d97727bb0b7_2024218</td>\n",
       "      <td>37a554ee-fc49-4730-819c-2d97727bb0b7_2024218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024218</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>45365e18-6e38-48e7-b4a2-6b448b209034_2024218</td>\n",
       "      <td>45365e18-6e38-48e7-b4a2-6b448b209034_2024218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024219</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>21f8cb5d-f5ac-4a01-9287-43df5f6751a1_2024219</td>\n",
       "      <td>21f8cb5d-f5ac-4a01-9287-43df5f6751a1_2024219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024219</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>5d94f0d4-a1b1-4d18-8a62-591e196006a9_2024219</td>\n",
       "      <td>5d94f0d4-a1b1-4d18-8a62-591e196006a9_2024219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024219</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>92c78261-0f01-4d4f-8ae3-7aa875d6cd7b_2024219</td>\n",
       "      <td>92c78261-0f01-4d4f-8ae3-7aa875d6cd7b_2024219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024220</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>287389c4-4c48-4483-87c0-6b363b57bde2_2024220</td>\n",
       "      <td>287389c4-4c48-4483-87c0-6b363b57bde2_2024220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024220</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>50b15e40-9860-4574-8ab8-0bd960fe27de_2024220</td>\n",
       "      <td>50b15e40-9860-4574-8ab8-0bd960fe27de_2024220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024220</td>\n",
       "      <td>ITM 3.20.2024 405F</td>\n",
       "      <td>9d0a4033-4482-4a17-833b-e410bf0d178e_2024220</td>\n",
       "      <td>9d0a4033-4482-4a17-833b-e410bf0d178e_2024220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2024221</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>6666ee6b-863b-49d1-8097-97e6aa4fb39d_2024221</td>\n",
       "      <td>6666ee6b-863b-49d1-8097-97e6aa4fb39d_2024221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2024221</td>\n",
       "      <td>ITM 3.20.2024 405E</td>\n",
       "      <td>6a76af00-56ef-408f-a445-6f5168e090a8_2024221</td>\n",
       "      <td>6a76af00-56ef-408f-a445-6f5168e090a8_2024221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024222</td>\n",
       "      <td>ITM 3.22.2024</td>\n",
       "      <td>c99de80f-15cc-45cb-aa64-5af0f2f118ca_2024222</td>\n",
       "      <td>c99de80f-15cc-45cb-aa64-5af0f2f118ca_2024222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2024223</td>\n",
       "      <td>ITM 3.22.2024</td>\n",
       "      <td>70eef02d-d2d0-458e-a8bb-f6511bf47a0c_2024223</td>\n",
       "      <td>70eef02d-d2d0-458e-a8bb-f6511bf47a0c_2024223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024223</td>\n",
       "      <td>ITM 3.22.2024</td>\n",
       "      <td>c9a8dc60-f61d-44bb-bcb5-6e2466f3c9a0_2024223</td>\n",
       "      <td>c9a8dc60-f61d-44bb-bcb5-6e2466f3c9a0_2024223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id             folder0  \\\n",
       "0          2024201  ITM 3.14.2024 405F   \n",
       "1          2024201  ITM 3.14.2024 405F   \n",
       "2          2024201  ITM 3.14.2024 405F   \n",
       "3          2024202  ITM 3.14.2024 405E   \n",
       "4          2024202  ITM 3.14.2024 405E   \n",
       "5          2024203  ITM 3.14.2024 405E   \n",
       "6          2024203  ITM 3.14.2024 405E   \n",
       "7          2024204  ITM 3.14.2024 405E   \n",
       "8          2024204  ITM 3.14.2024 405E   \n",
       "9          2024205  ITM 3.14.2024 405F   \n",
       "10         2024205  ITM 3.14.2024 405F   \n",
       "11         2024206  ITM 3.14.2024 405E   \n",
       "12         2024207  ITM 3.14.2024 405F   \n",
       "13         2024207  ITM 3.14.2024 405F   \n",
       "14         2024207  ITM 3.14.2024 405F   \n",
       "15         2024208  ITM 3.14.2024 405E   \n",
       "16         2024208  ITM 3.14.2024 405E   \n",
       "17         2024208  ITM 3.14.2024 405E   \n",
       "18         2024209  ITM 3.14.2024 405E   \n",
       "19         2024212  ITM 3.20.2024 405F   \n",
       "20         2024212  ITM 3.20.2024 405F   \n",
       "21         2024212  ITM 3.20.2024 405F   \n",
       "22         2024214  ITM 3.20.2024 405E   \n",
       "23         2024214  ITM 3.20.2024 405E   \n",
       "24         2024214  ITM 3.20.2024 405E   \n",
       "25         2024214  ITM 3.20.2024 405E   \n",
       "26         2024215  ITM 3.20.2024 405F   \n",
       "27         2024215  ITM 3.20.2024 405F   \n",
       "28         2024215  ITM 3.20.2024 405F   \n",
       "29         2024215  ITM 3.20.2024 405F   \n",
       "30         2024216  ITM 3.20.2024 405E   \n",
       "31         2024216  ITM 3.20.2024 405E   \n",
       "32         2024217  ITM 3.20.2024 405F   \n",
       "33         2024217  ITM 3.20.2024 405F   \n",
       "34         2024217  ITM 3.20.2024 405F   \n",
       "35         2024218  ITM 3.20.2024 405E   \n",
       "36         2024218  ITM 3.20.2024 405E   \n",
       "37         2024219  ITM 3.20.2024 405F   \n",
       "38         2024219  ITM 3.20.2024 405F   \n",
       "39         2024219  ITM 3.20.2024 405F   \n",
       "40         2024220  ITM 3.20.2024 405F   \n",
       "41         2024220  ITM 3.20.2024 405F   \n",
       "42         2024220  ITM 3.20.2024 405F   \n",
       "43         2024221  ITM 3.20.2024 405E   \n",
       "44         2024221  ITM 3.20.2024 405E   \n",
       "45         2024222       ITM 3.22.2024   \n",
       "46         2024223       ITM 3.22.2024   \n",
       "47         2024223       ITM 3.22.2024   \n",
       "\n",
       "                                         folder1  \\\n",
       "0   23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201   \n",
       "1   23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201   \n",
       "2   bccb0095-5efd-4c5c-ad58-8b8624f9ab56_2024201   \n",
       "3   220b609b-0e35-454e-9afd-c84cbfa3e3ad_2024202   \n",
       "4   922ad146-241a-4ea6-8ff1-413d7e0d16ec_2024202   \n",
       "5   4bc46c8c-66e7-463d-b3a1-2a8303af4fd1_2024203   \n",
       "6   acf74a81-a534-44c7-9cb1-67ec381b5ee0_2024203   \n",
       "7   80f79d45-22fd-479d-b6e2-c62b5778e073_2024204   \n",
       "8   cbbf410f-4657-428e-9616-8a777cc4704d_2024204   \n",
       "9   8cc40587-ff7d-4ebd-98ee-6441baedf7e0_2024205   \n",
       "10  df2fcf88-874b-4cf9-9707-3fa0b30c348f_2024205   \n",
       "11  499179ba-3138-4bae-918e-ffc7fb943760_2024206   \n",
       "12  04f80090-9e61-431d-8473-dccb75fed04d_2024207   \n",
       "13  66c2b794-b3b7-4d8a-a9fa-0e3338931eab_2024207   \n",
       "14  8839e3b8-be5e-4878-8aaf-26c656ae2270_2024207   \n",
       "15  0bc62b93-ac4d-40ef-bdfe-1d7badb24f70_2024208   \n",
       "16  67dc0230-511d-41ac-ae9b-850900ab9e6a_2024208   \n",
       "17  bceedc17-7d27-42bd-aa14-d6a98f16c6a5_2024208   \n",
       "18  b5989edc-8348-4b84-b649-87fc4f1cca53_2024209   \n",
       "19  2e8f6555-a7fa-4b54-8132-c030d697b4ad_2024212   \n",
       "20  5a909dc4-0f86-430d-a400-e37139cba69f_2024212   \n",
       "21  a1bcac92-c01f-485a-9ce0-8198739b2c27_2024212   \n",
       "22  0c0cc880-b3fb-488d-a468-0e67c17ca176_2024214   \n",
       "23  0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492_2024214   \n",
       "24  12988238-24b6-4ac2-9f33-398321e82ae0_2024214   \n",
       "25  d2aaf0ef-a32f-4255-b3f5-56df927ae0b4_2024214   \n",
       "26  3cf14e31-f416-4c78-8a69-91bf0c685448_2024215   \n",
       "27  56d09dbd-9f4b-41e8-bfb3-143832e61d94_2024215   \n",
       "28  7e23cc31-422a-42e1-acb5-964c661750f4_2024215   \n",
       "29  a7ce6f7b-6466-4281-9496-92b640d9d04b_2024215   \n",
       "30  c6d3a90f-68c0-4948-bd96-537e80973605_2024216   \n",
       "31  d13091cc-98e4-4aba-8d02-7eca8bd1a30c_2024216   \n",
       "32  1995e7ef-ef02-4fc1-b1ab-f137dbf69d48_2024217   \n",
       "33  6db9446c-2cd4-41b4-be8d-be5ccbbc6e05_2024217   \n",
       "34  c23df057-ab53-4276-b791-ebe38c431df7_2024217   \n",
       "35  37a554ee-fc49-4730-819c-2d97727bb0b7_2024218   \n",
       "36  45365e18-6e38-48e7-b4a2-6b448b209034_2024218   \n",
       "37  21f8cb5d-f5ac-4a01-9287-43df5f6751a1_2024219   \n",
       "38  5d94f0d4-a1b1-4d18-8a62-591e196006a9_2024219   \n",
       "39  92c78261-0f01-4d4f-8ae3-7aa875d6cd7b_2024219   \n",
       "40  287389c4-4c48-4483-87c0-6b363b57bde2_2024220   \n",
       "41  50b15e40-9860-4574-8ab8-0bd960fe27de_2024220   \n",
       "42  9d0a4033-4482-4a17-833b-e410bf0d178e_2024220   \n",
       "43  6666ee6b-863b-49d1-8097-97e6aa4fb39d_2024221   \n",
       "44  6a76af00-56ef-408f-a445-6f5168e090a8_2024221   \n",
       "45  c99de80f-15cc-45cb-aa64-5af0f2f118ca_2024222   \n",
       "46  70eef02d-d2d0-458e-a8bb-f6511bf47a0c_2024223   \n",
       "47  c9a8dc60-f61d-44bb-bcb5-6e2466f3c9a0_2024223   \n",
       "\n",
       "                                         folder2  \n",
       "0   1241b7f0-5262-4dc2-9102-2425b9b42864_2024201  \n",
       "1   23081f6e-875e-44f5-8bd0-edc3905f5c2c_2024201  \n",
       "2   bccb0095-5efd-4c5c-ad58-8b8624f9ab56_2024201  \n",
       "3   220b609b-0e35-454e-9afd-c84cbfa3e3ad_2024202  \n",
       "4   922ad146-241a-4ea6-8ff1-413d7e0d16ec_2024202  \n",
       "5   4bc46c8c-66e7-463d-b3a1-2a8303af4fd1_2024203  \n",
       "6   acf74a81-a534-44c7-9cb1-67ec381b5ee0_2024203  \n",
       "7   80f79d45-22fd-479d-b6e2-c62b5778e073_2024204  \n",
       "8   cbbf410f-4657-428e-9616-8a777cc4704d_2024204  \n",
       "9   8cc40587-ff7d-4ebd-98ee-6441baedf7e0_2024205  \n",
       "10  df2fcf88-874b-4cf9-9707-3fa0b30c348f_2024205  \n",
       "11  499179ba-3138-4bae-918e-ffc7fb943760_2024206  \n",
       "12  04f80090-9e61-431d-8473-dccb75fed04d_2024207  \n",
       "13  66c2b794-b3b7-4d8a-a9fa-0e3338931eab_2024207  \n",
       "14  8839e3b8-be5e-4878-8aaf-26c656ae2270_2024207  \n",
       "15  0bc62b93-ac4d-40ef-bdfe-1d7badb24f70_2024208  \n",
       "16  67dc0230-511d-41ac-ae9b-850900ab9e6a_2024208  \n",
       "17  bceedc17-7d27-42bd-aa14-d6a98f16c6a5_2024208  \n",
       "18  b5989edc-8348-4b84-b649-87fc4f1cca53_2024209  \n",
       "19  2e8f6555-a7fa-4b54-8132-c030d697b4ad_2024212  \n",
       "20  5a909dc4-0f86-430d-a400-e37139cba69f_2024212  \n",
       "21  a1bcac92-c01f-485a-9ce0-8198739b2c27_2024212  \n",
       "22  0c0cc880-b3fb-488d-a468-0e67c17ca176_2024214  \n",
       "23  0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492_2024214  \n",
       "24  12988238-24b6-4ac2-9f33-398321e82ae0_2024214  \n",
       "25  d2aaf0ef-a32f-4255-b3f5-56df927ae0b4_2024214  \n",
       "26  3cf14e31-f416-4c78-8a69-91bf0c685448_2024215  \n",
       "27  56d09dbd-9f4b-41e8-bfb3-143832e61d94_2024215  \n",
       "28  7e23cc31-422a-42e1-acb5-964c661750f4_2024215  \n",
       "29  a7ce6f7b-6466-4281-9496-92b640d9d04b_2024215  \n",
       "30  c6d3a90f-68c0-4948-bd96-537e80973605_2024216  \n",
       "31  d13091cc-98e4-4aba-8d02-7eca8bd1a30c_2024216  \n",
       "32  1995e7ef-ef02-4fc1-b1ab-f137dbf69d48_2024217  \n",
       "33  6db9446c-2cd4-41b4-be8d-be5ccbbc6e05_2024217  \n",
       "34  c23df057-ab53-4276-b791-ebe38c431df7_2024217  \n",
       "35  37a554ee-fc49-4730-819c-2d97727bb0b7_2024218  \n",
       "36  45365e18-6e38-48e7-b4a2-6b448b209034_2024218  \n",
       "37  21f8cb5d-f5ac-4a01-9287-43df5f6751a1_2024219  \n",
       "38  5d94f0d4-a1b1-4d18-8a62-591e196006a9_2024219  \n",
       "39  92c78261-0f01-4d4f-8ae3-7aa875d6cd7b_2024219  \n",
       "40  287389c4-4c48-4483-87c0-6b363b57bde2_2024220  \n",
       "41  50b15e40-9860-4574-8ab8-0bd960fe27de_2024220  \n",
       "42  9d0a4033-4482-4a17-833b-e410bf0d178e_2024220  \n",
       "43  6666ee6b-863b-49d1-8097-97e6aa4fb39d_2024221  \n",
       "44  6a76af00-56ef-408f-a445-6f5168e090a8_2024221  \n",
       "45  c99de80f-15cc-45cb-aa64-5af0f2f118ca_2024222  \n",
       "46  70eef02d-d2d0-458e-a8bb-f6511bf47a0c_2024223  \n",
       "47  c9a8dc60-f61d-44bb-bcb5-6e2466f3c9a0_2024223  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check out the various sub directories associated with participant JSON files\n",
    "rows_list = []\n",
    "for participant_id in range(2_024_201, 2_024_223+1):\n",
    "    for sub_directory, directories_list, files_list in os.walk(logs_path):\n",
    "        for file_name in files_list:\n",
    "            if file_name.endswith('.json') and ('_' in file_name):\n",
    "                if (file_name.split('_')[1].split('.')[0] == str(participant_id)):\n",
    "                    directory_suffix = sub_directory.replace('../data/logs/Metrics Evaluation Open World/', '')\n",
    "                    folder_names_list = re.split('/', directory_suffix, 0)\n",
    "                    row_dict = {'participant_id': participant_id}\n",
    "                    for i, folder_name in enumerate(folder_names_list): row_dict[f'folder{i}'] = folder_name\n",
    "                    rows_list.append(row_dict)\n",
    "df = DataFrame(rows_list)\n",
    "folders_df = DataFrame([])\n",
    "groupby_columns = ['participant_id', 'folder0']\n",
    "for (participant_id, folder0), folder0_df in df.groupby(groupby_columns):\n",
    "    folders_df = pd.concat([folders_df, folder0_df], axis='index').reset_index(drop=True)\n",
    "folders_df.sort_values(['participant_id'] + [f'folder{i}' for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52447196-d7c9-4c5d-aa9f-efbf887e376a",
   "metadata": {},
   "source": [
    "\n",
    "## Create Tag Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2e3696-ab61-4ab5-ab18-d000a1bf10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through each patient of each scene of each session of the 11-patient data frame\n",
    "rows_list = []\n",
    "for encounter_layout, responder_categories_df in elevens_df.groupby('encounter_layout'):\n",
    "    for (session_uuid, scene_id, patient_id), patient_df in responder_categories_df.sort_values(['action_tick']).groupby(fu.patient_groupby_columns):\n",
    "\n",
    "        # Get non-null tag applied types and patient record SALTs\n",
    "        mask_series = ~patient_df.tag_applied_type.isnull() | ~patient_df.patient_record_salt.isnull()\n",
    "        if mask_series.any():\n",
    "            tags_and_salts_df = patient_df[mask_series]\n",
    "\n",
    "            # Add the groupby columns and an account of the patient's existence to the row dictionary\n",
    "            row_dict = {cn: eval(cn) for cn in fu.patient_groupby_columns}\n",
    "            row_dict['encounter_layout'] = encounter_layout\n",
    "            for cn in participant_columns:\n",
    "                cv = patient_df[cn].mode().squeeze()\n",
    "                if not isinstance(cv, pd.Series): row_dict[cn] = cv\n",
    "            row_dict['patient_count'] = 1\n",
    "\n",
    "            # Add the TAG_APPLIED tag value for this patient\n",
    "            last_tag = fu.get_last_tag(tags_and_salts_df)\n",
    "            row_dict['last_tag'] = last_tag\n",
    "\n",
    "            # Add the PATIENT_RECORD SALT value for this patient\n",
    "            max_salt = fu.get_max_salt(patient_df=tags_and_salts_df)\n",
    "            row_dict['max_salt'] = max_salt\n",
    "\n",
    "            # Add the predicted tag value for this patient based on the SALT value\n",
    "            try: predicted_tag = fu.salt_to_tag_dict.get(max_salt, np.nan)\n",
    "            except Exception: predicted_tag = np.nan\n",
    "            row_dict['predicted_tag'] = predicted_tag\n",
    "\n",
    "            # Add if the tagging was correct for this patient, then the row to the list\n",
    "            row_dict['is_tag_correct'] = bool(last_tag == predicted_tag)\n",
    "            rows_list.append(row_dict)\n",
    "\n",
    "# Create the tag-to-SALT data frame\n",
    "tag_to_salt_df = pd.DataFrame(rows_list)\n",
    "\n",
    "# Convert the tagged, SALT, and predicted tag columns to their custom categorical types\n",
    "tag_to_salt_df.last_tag = tag_to_salt_df.last_tag.astype(fu.colors_category_order)\n",
    "tag_to_salt_df.max_salt = tag_to_salt_df.max_salt.astype(fu.salt_category_order)\n",
    "tag_to_salt_df.predicted_tag = tag_to_salt_df.predicted_tag.astype(fu.colors_category_order)\n",
    "\n",
    "# Sort the data frame based on the custom categorical orders\n",
    "tag_to_salt_df = tag_to_salt_df.sort_values('predicted_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3aada9-5b98-478c-b24c-ac81c3fec285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the total and correct counts for each run for each tag\n",
    "rows_list = []\n",
    "\n",
    "# Add the normal section\n",
    "groupby_columns = ['session_uuid', 'scene_id', 'predicted_tag']\n",
    "for encounter_layout, responder_categories_df in tag_to_salt_df.groupby('encounter_layout'):\n",
    "    for (session_uuid, scene_id, predicted_tag), df in responder_categories_df.groupby(groupby_columns):\n",
    "        \n",
    "        # Add the groupby columns to the row dictionary\n",
    "        row_dict = {cn: eval(cn) for cn in groupby_columns}\n",
    "        row_dict['encounter_layout'] = encounter_layout\n",
    "        for cn in participant_columns:\n",
    "            cv = df[cn].mean()\n",
    "            if not isinstance(cv, pd.Series): row_dict['mean_' + cn] = cv\n",
    "\n",
    "        # Add the total and correct counts for this run\n",
    "        mask_series = (df.is_tag_correct == True)\n",
    "        correct_count = df[mask_series].patient_count.sum()\n",
    "        row_dict['correct_count'] = correct_count\n",
    "        total_count = df.patient_count.sum()\n",
    "        row_dict['total_count'] = total_count\n",
    "\n",
    "        # Add percentage that tag is correct\n",
    "        try: percentage_tag_correct = 100*correct_count/total_count\n",
    "        except Exception: percentage_tag_correct = np.nan\n",
    "        row_dict['percentage_tag_correct'] = percentage_tag_correct\n",
    "\n",
    "        # Add the row dictionary to the list\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "    # Add the not-tagged section\n",
    "    for (session_uuid, scene_id), df in responder_categories_df.groupby(fu.scene_groupby_columns):\n",
    "\n",
    "        # Add the groupby columns to the row dictionary\n",
    "        row_dict = {cn: eval(cn) for cn in fu.scene_groupby_columns}\n",
    "        row_dict['predicted_tag'] = 'Not Tagged'\n",
    "        row_dict['encounter_layout'] = encounter_layout\n",
    "        for cn in participant_columns:\n",
    "            cv = df[cn].mean()\n",
    "            if not isinstance(cv, pd.Series): row_dict['mean_' + cn] = cv\n",
    "\n",
    "        # Add the total and correct counts for this run\n",
    "        mask_series = (df.is_tag_correct == True)\n",
    "        correct_count = df[mask_series].patient_count.sum()\n",
    "        row_dict['correct_count'] = correct_count\n",
    "        total_count = df.patient_count.sum()\n",
    "        row_dict['total_count'] = total_count\n",
    "\n",
    "        # Add percentage that tag is correct\n",
    "        try: percentage_tag_correct = 100*correct_count/total_count\n",
    "        except Exception: percentage_tag_correct = np.nan\n",
    "        row_dict['percentage_tag_correct'] = percentage_tag_correct\n",
    "\n",
    "        # Add the row dictionary to the list\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "# Create the correct count data frame\n",
    "correct_count_by_tag_df = pd.DataFrame(rows_list)\n",
    "correct_count_by_tag_df.predicted_tag = correct_count_by_tag_df.predicted_tag.astype(fu.colors_category_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5115db6-335a-4c88-a60d-af6bfb44f2ed",
   "metadata": {},
   "source": [
    "\n",
    "<h2>We only want to use data from the following characters within each csv (by environment):</h2>\n",
    "<h3>Desert:</h3>\n",
    "<ul>\n",
    "    <li>Open World Marine 1 Female</li>\n",
    "    <li>Open World Marine 2 Male</li>\n",
    "    <li>Open World Civilian 1 Male</li>\n",
    "    <li>Open World Civilian 2 Female</li>\n",
    "</ul>\n",
    "<h3>Jungle:</h3>\n",
    "<ul>\n",
    "    <li>Open World Marine 1 Male</li>\n",
    "    <li>Open World Marine 2 Female</li>\n",
    "    <li>Open World Marine 3 Male</li>\n",
    "    <li>Open World Marine 4 Male</li>\n",
    "</ul>\n",
    "<h3>Submarine:</h3>\n",
    "<ul>\n",
    "    <li>Navy Soldier 1 Male</li>\n",
    "    <li>Navy Soldier 2 Male</li>\n",
    "    <li>Navy Soldier 3 Male</li>\n",
    "    <li>Navy Soldier 4 Female</li>\n",
    "</ul>\n",
    "<h3>Urban:</h3>\n",
    "<ul>\n",
    "    <li>Marine 1 Male</li>\n",
    "    <li>Marine 2 Male</li>\n",
    "    <li>Marine 3 Male</li>\n",
    "    <li>Marine 4 Male</li>\n",
    "    <li>Civilian 1 Female</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31f355b-4033-4683-a770-f17c5457d62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 49)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all patient stats\n",
    "from numpy import nan\n",
    "\n",
    "rows_list = []\n",
    "for (encounter_layout, session_uuid, scene_id, patient_id), patient_df in elevens_df.groupby(['encounter_layout'] + fu.patient_groupby_columns):\n",
    "    row_dict = {cn: eval(cn) for cn in ['encounter_layout'] + fu.patient_groupby_columns}\n",
    "    for cn in participant_columns:\n",
    "        cv = patient_df[cn].mode().squeeze()\n",
    "        if not isinstance(cv, pd.Series): row_dict[cn] = cv\n",
    "    \n",
    "    row_dict['correct_bleeding_tool_applied'] = fu.get_is_correct_bleeding_tool_applied(patient_df)\n",
    "    row_dict['first_patient_interaction'] = fu.get_first_patient_interaction(patient_df)\n",
    "    row_dict['last_patient_interaction'] = fu.get_last_patient_interaction(patient_df)\n",
    "    row_dict['last_tag'] = fu.get_last_tag(patient_df)\n",
    "    row_dict['life_threatened'] = fu.get_is_life_threatened(patient_df)\n",
    "    row_dict['max_salt'] = fu.get_max_salt(patient_df)\n",
    "    row_dict['maximum_injury_severity'] = fu.get_maximum_injury_severity(patient_df)\n",
    "    row_dict['patient_dead'] = fu.get_is_patient_dead(patient_df)\n",
    "    row_dict['patient_engagement_count'] = fu.get_patient_engagement_count(patient_df)\n",
    "    row_dict['patient_gazed_at'] = fu.get_is_patient_gazed_at(patient_df)\n",
    "    row_dict['patient_hemorrhaging'] = fu.get_is_patient_hemorrhaging(patient_df)\n",
    "    row_dict['patient_severely_hemorrhaging'] = fu.get_is_patient_severely_hemorrhaging(patient_df)\n",
    "    row_dict['patient_still'] = fu.get_is_patient_still(patient_df)\n",
    "    row_dict['pulse_value'] = fu.get_pulse_value(patient_df)\n",
    "    row_dict['tag_value'] = fu.get_tag_value(patient_df)\n",
    "    row_dict['time_to_hemorrhage_control'] = fu.get_time_to_hemorrhage_control(patient_df)\n",
    "    \n",
    "    mask_series = ~patient_df.tag_applied_type.isnull()\n",
    "    tag_applied_type_count = patient_df[mask_series].tag_applied_type.unique().shape[0]\n",
    "    mask_series = ~patient_df.patient_record_salt.isnull()\n",
    "    patient_record_salt_count = patient_df[mask_series].patient_record_salt.unique().shape[0]\n",
    "    if (tag_applied_type_count > 0) and (patient_record_salt_count > 0): row_dict['tag_correct'] = fu.get_is_tag_correct(patient_df)\n",
    "    else: row_dict['tag_correct'] = nan\n",
    "    \n",
    "    mask_series = patient_df.action_type.isin(fu.action_types_list)\n",
    "    row_dict['action_count'] = patient_df[mask_series].shape[0]\n",
    "    \n",
    "    mask_series = patient_df.action_type.isin(['PATIENT_ENGAGED', 'PULSE_TAKEN'])\n",
    "    row_dict['assessment_count'] = patient_df[mask_series].shape[0]\n",
    "    \n",
    "    mask_series = patient_df.action_type.isin(['INJURY_TREATED'])\n",
    "    row_dict['treatment_count'] = patient_df[mask_series].shape[0]\n",
    "    \n",
    "    mask_series = patient_df.action_type.isin(['TAG_APPLIED'])\n",
    "    row_dict['tag_application_count'] = patient_df[mask_series].shape[0]\n",
    "    \n",
    "    if (row_dict['max_salt'] == 'EXPECTANT'):\n",
    "        mask_series = ~patient_df.injury_treated_required_procedure.isnull() | ~patient_df.tool_applied_type.isnull()\n",
    "        row_dict['treated_expectant'] = {True: 'yes', False: 'no'}[mask_series.any()]\n",
    "    else: row_dict['treated_expectant'] = nan\n",
    "    \n",
    "    rows_list.append(row_dict)\n",
    "patient_stats_df = DataFrame(rows_list)\n",
    "patient_stats_df.max_salt = patient_stats_df.max_salt.astype(fu.salt_category_order)\n",
    "patient_stats_df.last_tag = patient_stats_df.last_tag.astype(fu.colors_category_order)\n",
    "column_descriptions_df = nu.get_column_descriptions(patient_stats_df)\n",
    "# mask_series = (df.min_value == False) & (df.max_value == True) & (df.dtype == 'object')\n",
    "# for cn in df[mask_series].column_name: patient_stats_df[cn] = patient_stats_df[cn].astype(bool)\n",
    "print(patient_stats_df.shape) # (415, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a21fff-9c0a-4278-a7d8-d0658a8d7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a to-be-grouped summary of scene-based statistics\n",
    "desert_patients_list = [\n",
    "    'Open World Marine 1 Female Root', 'Open World Marine 2 Male Root', 'Open World Civilian 1 Male Root', 'Open World Civilian 2 Female Root'\n",
    "]\n",
    "jungle_patients_list = ['Open World Marine 1 Male Root', 'Open World Marine 2 Female Root', 'Open World Marine 3 Male Root', 'Open World Marine 4 Male Root']\n",
    "submarine_patients_list = ['Navy Soldier 1 Male Root', 'Navy Soldier 2 Male Root', 'Navy Soldier 3 Male Root', 'Navy Soldier 4 Female Root']\n",
    "urban_patients_list = ['Marine 1 Male Root', 'Marine 2 Male Root', 'Marine 3 Male Root', 'Marine 4 Male Root', 'Civilian 1 Female Root']\n",
    "anova_rows_list = []\n",
    "for encounter_layout, encounter_layout_df in patient_stats_df.groupby('encounter_layout'):\n",
    "    patients_list = eval(f'{encounter_layout.lower()}_patients_list')\n",
    "    for (session_uuid, scene_id), scene_df in encounter_layout_df.groupby(fu.scene_groupby_columns):\n",
    "        mask_series = scene_df.patient_id.isin(patients_list)\n",
    "        if mask_series.any():\n",
    "            row_dict = {'Environment': encounter_layout, 'session_uuid': session_uuid, 'scene_id': scene_id}\n",
    "            for cn in participant_columns:\n",
    "                cv = patient_df[cn].mode().squeeze()\n",
    "                if not isinstance(cv, pd.Series): row_dict[cn] = cv\n",
    "\n",
    "            # Get the start of the whole scene\n",
    "            scene_mask_series = True\n",
    "            for cn in fu.scene_groupby_columns: scene_mask_series &= (elevens_df[cn] == eval(cn))\n",
    "            scene_start = fu.get_scene_start(elevens_df[scene_mask_series])\n",
    "            \n",
    "            row_dict['mean_first_patient_interaction'] = scene_df.first_patient_interaction.map(lambda x: x - scene_start).mean()\n",
    "            row_dict['mean_last_patient_interaction'] = scene_df.last_patient_interaction.map(lambda x: x - scene_start).mean()\n",
    "            row_dict['total_patient_engagement_count'] = scene_df.patient_engagement_count.sum()\n",
    "            row_dict['total_action_count'] = scene_df.action_count.sum()\n",
    "            row_dict['total_assessment_count'] = scene_df.assessment_count.sum()\n",
    "            row_dict['total_treatment_count'] = scene_df.treatment_count.sum()\n",
    "            row_dict['total_tag_application_count'] = scene_df.tag_application_count.sum()\n",
    "            row_dict['max_time_to_hemorrhage_control'] = scene_df.time_to_hemorrhage_control.max()\n",
    "\n",
    "            mask_series = (scene_df.treated_expectant == 'yes')\n",
    "            row_dict['treated_expectant_count'] = mask_series.sum()\n",
    "\n",
    "            # Get the whole scene history\n",
    "            scene_df = elevens_df[scene_mask_series]\n",
    "            mask_series = scene_df.patient_id.isin(patients_list)\n",
    "            \n",
    "            row_dict['total_time_to_triage_scene'] = fu.get_triage_time(scene_df[mask_series])\n",
    "            row_dict['time_to_hemorrhage_control'] = fu.get_time_to_last_hemorrhage_controlled(scene_df[mask_series])\n",
    "            times_list = []\n",
    "            for patient_id, patient_df in scene_df[mask_series].groupby('patient_id'):\n",
    "                if fu.get_is_patient_hemorrhaging(patient_df):\n",
    "                    controlled_time = fu.get_time_to_hemorrhage_control(patient_df, scene_start=None)\n",
    "                    times_list.append(controlled_time)\n",
    "            row_dict['mean_time_for_hemorrhage_control_per_patient'] = Series(times_list).mean()\n",
    "            mask_series = (correct_count_by_tag_df.session_uuid == session_uuid) & (correct_count_by_tag_df.scene_id == scene_id)\n",
    "            if mask_series.any():\n",
    "                row_dict['mean_percent_accurate_tagging'] = correct_count_by_tag_df[mask_series].percentage_tag_correct.mean()\n",
    "            \n",
    "            anova_rows_list.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3af14-8e96-4885-9e66-1a42133e3dce",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Here is my initial list but I am open to suggestions and modifications</h2>\n",
    "<h3>IVs (these are not available in the csv; we are working on calculating them now and can get you that info.)</h3>\n",
    "<ul>\n",
    "    <li>Participant medical role</li>\n",
    "    <li>Years of experience</li>\n",
    "    <li>ST alignment score (continuous or group assignment)</li>\n",
    "    <li>AD alignment score (continuous or group assignment)</li>\n",
    "</ul>\n",
    "<h3>DVs</h3>\n",
    "<ul>\n",
    "    <li>Total number of actions</li>\n",
    "    <li>Count of assessment actions</li>\n",
    "    <li>Count of treatment actions</li>\n",
    "    <li>Count of tags applied</li>\n",
    "    <li>Order of patients engaged</li>\n",
    "    <li>Tag color for each patient</li>\n",
    "    <li>Treat expectant patient (yes/no)</li>\n",
    "    <li>Triage efficiency</li>\n",
    "    <li>Time to hemorrhage control</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ad4986-b2d3-4902-801e-d1138f7fc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attempt to manufacture some better column names\n",
    "from pandas import read_excel\n",
    "\n",
    "file_path = '../data/xlsx/Metrics_Evaluation_Dataset_organization_for_BBAI.xlsx'\n",
    "dataset_organization_df = read_excel(file_path)\n",
    "\n",
    "mask_series = ~dataset_organization_df.Description.isnull()#re.sub('[^A-Za-z0-9]+', '_', x)\n",
    "df = dataset_organization_df[mask_series]\n",
    "description_dict = df.set_index('Variable').Description.map(lambda x: ' (' + x + ')').to_dict()\n",
    "new_description_dict = description_dict.copy()\n",
    "for k, v in description_dict.items():\n",
    "    new_description_dict[k] = v\n",
    "    new_description_dict[f'{k}_Text'] = v\n",
    "description_dict = new_description_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbef0d-7fc8-403c-b74c-4f12c09d8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "column_descriptions_df = nu.get_column_descriptions(anova_df)\n",
    "mask_series = column_descriptions_df.dtype.isin(['float64', 'int64'])\n",
    "columns_set = set(column_descriptions_df[mask_series].column_name).difference(set(['scene_id', 'const']))\n",
    "df = anova_df[participant_columns].corr()\n",
    "mask_series = df.applymap(lambda x: abs(x) > 0.5)\n",
    "for cn in df[mask_series].columns:\n",
    "    columns_list = df[mask_series][cn].dropna().index.tolist()\n",
    "    if len(columns_list) > 1: PairGrid_obj = sns.pairplot(anova_df[columns_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0384e2b-c9a9-4a7f-8a77-06dfb4068704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Which factors contribute to the variance in these outcomes?\n",
    "# Set up a regression equation (it will be continuous between 0 and 1) as the outcome\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "import itertools\n",
    "from scipy.stats import kruskal\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Get the numeric columns\n",
    "anova_df = DataFrame(anova_rows_list)\n",
    "column_descriptions_df = nu.get_column_descriptions(anova_df)\n",
    "mask_series = column_descriptions_df.dtype.isin(['float64', 'int64'])\n",
    "columns_set = set(column_descriptions_df[mask_series].column_name).difference(set(['scene_id']))\n",
    "\n",
    "for groupby_column in columns_set:\n",
    "    \n",
    "    # Encode the group-by column into dummy variables\n",
    "    dummy_groupby = pd.get_dummies(anova_df[groupby_column], prefix=groupby_column)\n",
    "    \n",
    "    # Concatenate the dummy variables with the dataframe\n",
    "    anova_with_dummies_df = pd.concat([anova_df, dummy_groupby], axis=1)\n",
    "    \n",
    "    # Add a constant term to the independent variables\n",
    "    anova_with_dummies_df['const'] = 1\n",
    "    \n",
    "    # Group data by groupby columns\n",
    "    grouped_data = anova_with_dummies_df.groupby(groupby_column)\n",
    "    \n",
    "    for cn in set(columns_set).difference(set([groupby_column])):\n",
    "        statements_list = []; display_box_and_whiskers = False; display_qq_plot = False; display_regression_equation = False\n",
    "        cn_data = anova_with_dummies_df[cn]\n",
    "        xname = ' '.join([w.title() for w in cn.split('_')])\n",
    "        statements_list.append(f\"\\n{xname}{description_dict.get(cn, '')} grouped by {groupby_column}{description_dict.get(groupby_column, '')}\")\n",
    "\n",
    "        # Perform a test for normality\n",
    "        try:\n",
    "            from scipy.stats import shapiro_wilk\n",
    "            \n",
    "            # Perform Shapiro-Wilk test\n",
    "            stat, p_value = shapiro_wilk(cn_data)\n",
    "            \n",
    "            # Append the test results\n",
    "            if not pd.isna(stat): statements_list.append(f'Shapiro-Wilk Test for normality ({cn}): Statistic: {stat:.4f}, p-value: {p_value:.4f}')\n",
    "    \n",
    "        except:\n",
    "            from scipy.stats import shapiro\n",
    "            \n",
    "            # Perform Shapiro-Francia test\n",
    "            stat, p_value = shapiro(cn_data)\n",
    "            \n",
    "            # Append the test results\n",
    "            if not pd.isna(stat): statements_list.append(f'Shapiro-Francia Test for normality ({cn}): Statistic: {stat:.4f}, p-value: {p_value:.4f}')\n",
    "\n",
    "        if grouped_data.ngroups > 1:\n",
    "            \n",
    "            # Fail to reject the null hypothesis of normality\n",
    "            if (p_value >= 0.05):\n",
    "                \n",
    "                # Perform ANOVA on values to get F statistic and p-value\n",
    "                f_statistic, p_value = f_oneway(*[group[cn] for name, group in grouped_data])\n",
    "                \n",
    "                # Append the results\n",
    "                if not pd.isna(p_value): statements_list.append(f\"One-way ANOVA for {cn}: F statistic = {f_statistic:.4f}, p-value = {p_value:.4f}\")\n",
    "        \n",
    "                # Calculate the sum of squares between groups (SS_between)\n",
    "                mean_overall = cn_data.mean()  # Mean of all values\n",
    "                SS_between = sum(len(group) * (group[cn].mean() - mean_overall)**2 for name, group in grouped_data)\n",
    "                \n",
    "                # Calculate the total sum of squares (SS_total)\n",
    "                SS_total = sum((value - mean_overall)**2 for name, group in grouped_data for value in group[cn])\n",
    "                \n",
    "                # Compute eta-squared (η²) and append the results\n",
    "                try: eta_squared = SS_between / SS_total\n",
    "                except: eta_squared = np.nan\n",
    "                if not pd.isna(eta_squared): statements_list.append(f\"    η² (effect size) = {eta_squared:.4f}\")\n",
    "                \n",
    "                # Generate theoretical quantiles from a standard normal distribution\n",
    "                theoretical_quantiles = norm.ppf(cn_data.rank() / (len(cn_data) + 1))\n",
    "    \n",
    "                display_qq_plot = True\n",
    "    \n",
    "            # The data does not come from a normal distribution: consider non-parametric tests\n",
    "            else:\n",
    "                \n",
    "                # Perform Kruskal-Wallis test on values\n",
    "                kruskal_statistic, p_value = kruskal(*[group[cn] for name, group in grouped_data])\n",
    "                \n",
    "                # Access additional test details from the result object\n",
    "                kruskal_results = kruskal(*[group[cn] for name, group in grouped_data])\n",
    "                \n",
    "                # Extract chi-squared statistic (equivalent to H statistic)\n",
    "                chi_squared = kruskal_results.statistic\n",
    "                \n",
    "                # Degrees of freedom (number of groups - 1)\n",
    "                degrees_of_freedom = len(anova_with_dummies_df[groupby_column].unique()) - 1\n",
    "                \n",
    "                # Append the complete results\n",
    "                statements_list.append(f\"Kruskal-Wallis test for {cn}: H statistic = {kruskal_statistic:.4f}, p-value = {p_value:.4f}\")\n",
    "                statements_list.append(f\"    χ² = {chi_squared:.4f}, df = {degrees_of_freedom}, p-value = {p_value:.4f}\")\n",
    "                \n",
    "                # Epsilon squared (effect size)\n",
    "                n = len(anova_with_dummies_df)  # Total number of observations\n",
    "                k = len(anova_with_dummies_df[groupby_column].unique())  # Number of groups\n",
    "            \n",
    "                # Calculate manually\n",
    "                try:\n",
    "                    ss_between = kruskal_results.statistic * n / (k * (k - 1))\n",
    "                    ss_total = kruskal_results.ss  # Access total sum of squares from results\n",
    "                    epsilon_squared = ss_between / ss_total\n",
    "                \n",
    "                # Alternative way to calculate sum of squares within groups (SSwithin)\n",
    "                except:\n",
    "                    data_by_env = grouped_data\n",
    "                    ss_within = sum([group[cn].var(ddof=0) * len(group) for name, group in data_by_env])\n",
    "                    \n",
    "                    # Total sum of squares (SStotal) - using variance of entire data\n",
    "                    ss_total = cn_data.var(ddof=0) * len(anova_with_dummies_df)\n",
    "                    \n",
    "                    # Epsilon squared calculation\n",
    "                    epsilon_squared = (kruskal_results.statistic * n) / (k * (k - 1) * ss_within / ss_total)\n",
    "    \n",
    "                # Append the results\n",
    "                if not pd.isna(epsilon_squared): statements_list.append(f\"    ε² (effect size) = {epsilon_squared:.4f}\")\n",
    "            \n",
    "            # Test shows a significant difference (p-value < 0.05)\n",
    "            if p_value < 0.05:\n",
    "                \n",
    "                # Compare the pairs\n",
    "                mask_series = ~anova_with_dummies_df[groupby_column].isnull()\n",
    "                for pair in itertools.combinations(anova_with_dummies_df[mask_series][groupby_column].unique(), 2):\n",
    "                    env1 = pair[0]; env2 = pair[1]\n",
    "                    env1_data = grouped_data.get_group(env1)[cn]\n",
    "                    env2_data = grouped_data.get_group(env2)[cn]\n",
    "                    t_statistic, p_value = ttest_ind(env1_data, env2_data)\n",
    "                    if p_value < 0.05: statements_list.append(f\"    t-test between {env1} and {env2}: t = {t_statistic:.4f}, p = {p_value:.4f}\")\n",
    "                \n",
    "                # Display a box and whiskers plot\n",
    "                mask_series = ~cn_data.isnull()\n",
    "                if mask_series.any():\n",
    "                    transformable_df = anova_with_dummies_df[mask_series]\n",
    "                    display_box_and_whiskers = True\n",
    "        \n",
    "        # Perform linear regression\n",
    "        columns_list = [cn for cn in anova_with_dummies_df.columns if cn.startswith(f'{groupby_column}_')]\n",
    "        model = sm.OLS(\n",
    "            cn_data, anova_with_dummies_df[['const'] + columns_list]\n",
    "        )\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Append the regression equation\n",
    "        if not pd.isna(results.params['const']):\n",
    "            display_regression_equation = True\n",
    "            statements_list.append(\"Regression Equation:\")\n",
    "            format_str = \"{} = {:.4f} + \" + ' + '.join([f'{{:.4f}} * {cn}' for cn in columns_list])\n",
    "            mask_series = (dataset_organization_df.Variable == groupby_column.replace('_Text', ''))\n",
    "            if mask_series.any():\n",
    "                equation_dict = {}\n",
    "                for kv in dataset_organization_df[mask_series].Labels:\n",
    "                    kv_split = re.split(' *= *', kv, 0)\n",
    "                    if len(kv_split) == 2: equation_dict[float(kv_split[0])] = re.sub('[^A-Za-z0-9]+', '_', kv_split[1]).strip('_')\n",
    "                for k, v in equation_dict.items(): format_str = format_str.replace('_' + str(k), '_' + v)\n",
    "            statements_list.append(format_str.format(\n",
    "                cn, results.params['const'], *[results.params[cn] for cn in columns_list]\n",
    "            ))\n",
    "        \n",
    "        if display_regression_equation and (cn in participant_columns):\n",
    "            print('\\n'.join(statements_list))\n",
    "            if display_qq_plot:\n",
    "                \n",
    "                # Create a figure and subplots with 1 row and 2 columns\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "                \n",
    "                # Create the QQ Plot\n",
    "                ax1.scatter(theoretical_quantiles, cn_data)\n",
    "                ax1.set_xlabel('Standard Normal Quantiles')\n",
    "                ax1.set_ylabel(f'{xname} Values')\n",
    "                ax1.set_title(f'QQ Plot of {xname}')\n",
    "                ax1.grid(True)\n",
    "                \n",
    "                # Create the Histogram\n",
    "                ax2 = nu.plot_histogram(\n",
    "                    anova_with_dummies_df, cn, xname, f'{xname} Histogram', xtick_text_fn=None, ylabel=None, xticks_are_temporal=False, ax=ax2,\n",
    "                    color=None, bins=100\n",
    "                )\n",
    "                \n",
    "                # Humanize y tick labels\n",
    "                yticklabels_list = []\n",
    "                for text_obj in ax2.get_yticklabels():\n",
    "                    position_tuple = text_obj.get_position()\n",
    "                    text_obj.set_text(int(position_tuple[1]))\n",
    "                    yticklabels_list.append(text_obj)\n",
    "                if (len(yticklabels_list) > 17): ax2.set_yticklabels(yticklabels_list, rotation=90)\n",
    "                else: ax2.set_yticklabels(yticklabels_list)\n",
    "                \n",
    "                # Adjust layout (optional)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Display the plot\n",
    "                plt.show()\n",
    "            \n",
    "            if display_box_and_whiskers:\n",
    "                fu.plot_grouped_box_and_whiskers(\n",
    "                    transformable_df,\n",
    "                    groupby_column,\n",
    "                    cn,\n",
    "                    groupby_column,\n",
    "                    ' '.join([w.title() for w in cn.split('_')]),\n",
    "                    transformer_name=None,\n",
    "                    is_y_temporal=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12328dac-0748-40ae-b39f-f6452e25e0d7",
   "metadata": {},
   "source": [
    "\n",
    "Generate a csv file with the following variables for each participant, with one variable (one for each environment) per column and one participant per row:\n",
    " 1. Count of teleports (TeleportCount_sub; TeleportCount_jungle; TeleportCount_desert; TeleportCount_urban)\n",
    " 2. Count of voice captures\n",
    " 3. Count of treatment applied (can you remove the pulse oximeter from this count?)\n",
    " 4. Count of pulse oximeter use\n",
    " 5. Total distance covered in the scene\n",
    " 6. Average time between engage patient and either teleport or engage next patient\n",
    " 7. Average time per patient (by using any actions that take a patient as a property like gaze and treat and engage)\n",
    " 8. Count of applied_tag\n",
    " 9. Apply_tag (expectant) 1 or 0\n",
    " 10. Count of pulse taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d97f2f90-a815-464b-b6b8-04da38a3c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['participantId', 'ParticipantID']\n",
      "(36, 2)\n",
      "['', nan, '2024208', '2024202', '2024206', '2024203', '2024204', '2024209', '2024207', '2024201', '2024205', '2024214', '2024218', '2024221', '2024216', '2024217', '2024219', '2024220', '2024212', '2024215', '2024222 ', '2024223', '2024222']\n",
      "[nan, 2024205.0, 2024201.0, 2024209.0, 2024207.0, 2024204.0, 2024203.0, 2024208.0, 2024202.0, 2024206.0, 2024211.0, 2024220.0, 2024221.0, 2024218.0, 2024215.0, 2024213.0, 2024219.0, 2024214.0, 2024217.0, 2024212.0, 2024216.0, 2024222.0, 2024223.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "      <th>count_blanks</th>\n",
       "      <th>count_uniques</th>\n",
       "      <th>count_zeroes</th>\n",
       "      <th>has_dates</th>\n",
       "      <th>min_value</th>\n",
       "      <th>max_value</th>\n",
       "      <th>only_integers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParticipantID</td>\n",
       "      <td>float64</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2024201.0</td>\n",
       "      <td>2024223.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>participantId</td>\n",
       "      <td>object</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>2024223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column_name    dtype  count_blanks  count_uniques  count_zeroes  \\\n",
       "0  ParticipantID  float64            10             23             0   \n",
       "1  participantId   object             5             23             0   \n",
       "\n",
       "   has_dates  min_value  max_value only_integers  \n",
       "0       True  2024201.0  2024223.0         False  \n",
       "1      False               2024223           NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Examine the participant ID columns\n",
    "columns_list = [cn for cn in file_stats_df.columns if 'partici' in cn.lower()]\n",
    "print(columns_list)\n",
    "df = file_stats_df[columns_list].drop_duplicates()\n",
    "print(df.shape)\n",
    "print(df.participantId.unique().tolist())\n",
    "print(df.ParticipantID.unique().tolist())\n",
    "column_descriptions_df = nu.get_column_descriptions(df)\n",
    "column_descriptions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52a65aea-f5a3-4e49-82bb-9fcfe5b00769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantId</th>\n",
       "      <th>ParticipantID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>2024207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024201</td>\n",
       "      <td>2024202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024202</td>\n",
       "      <td>2024201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024203</td>\n",
       "      <td>2024204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2024204</td>\n",
       "      <td>2024203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024205</td>\n",
       "      <td>2024206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024206</td>\n",
       "      <td>2024207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024207</td>\n",
       "      <td>2024208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024208</td>\n",
       "      <td>2024205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024209</td>\n",
       "      <td>2024209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2024212</td>\n",
       "      <td>2024212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2024212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2024214</td>\n",
       "      <td>2024215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2024215</td>\n",
       "      <td>2024216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2024215</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2024216</td>\n",
       "      <td>2024218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2024217</td>\n",
       "      <td>2024219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2024217</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2024218</td>\n",
       "      <td>2024220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2024219</td>\n",
       "      <td>2024214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2024219</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2024220</td>\n",
       "      <td>2024217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2024220</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2024221</td>\n",
       "      <td>2024221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2024222</td>\n",
       "      <td>2024222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2024222</td>\n",
       "      <td>2024222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2024223</td>\n",
       "      <td>2024223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participantId  ParticipantID\n",
       "34                    2024207.0\n",
       "0                           NaN\n",
       "39       2024201      2024202.0\n",
       "26       2024202      2024201.0\n",
       "29       2024203      2024204.0\n",
       "31       2024204      2024203.0\n",
       "45       2024205      2024206.0\n",
       "42       2024205            NaN\n",
       "28       2024206      2024207.0\n",
       "38       2024207      2024208.0\n",
       "40       2024207            NaN\n",
       "25       2024208      2024205.0\n",
       "35       2024209      2024209.0\n",
       "66       2024212      2024212.0\n",
       "77       2024212            NaN\n",
       "56       2024214      2024215.0\n",
       "47       2024214            NaN\n",
       "67       2024215      2024216.0\n",
       "69       2024215            NaN\n",
       "54       2024216      2024218.0\n",
       "63       2024217      2024219.0\n",
       "79       2024217            NaN\n",
       "49       2024218      2024220.0\n",
       "64       2024219      2024214.0\n",
       "75       2024219            NaN\n",
       "65       2024220      2024217.0\n",
       "76       2024220            NaN\n",
       "52       2024221      2024221.0\n",
       "84       2024222      2024222.0\n",
       "82      2024222       2024222.0\n",
       "83       2024223      2024223.0\n",
       "44           NaN      2024206.0\n",
       "27           NaN      2024209.0\n",
       "46           NaN      2024211.0\n",
       "58           NaN      2024213.0\n",
       "9            NaN            NaN"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Examine the correspondance between the columns\n",
    "df.sort_values(['participantId', 'ParticipantID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08df6f2a-ef33-4935-8d6f-16a87b057640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantId</th>\n",
       "      <th>ParticipantID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2024213.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participantId  ParticipantID\n",
       "46           NaN      2024211.0\n",
       "58           NaN      2024213.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Show any differences between the columns\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return not pd.isna(value)\n",
    "    except ValueError: pass\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError: return False\n",
    "\n",
    "id_set = set(\n",
    "    [int(el) for el in df.participantId.unique() if is_numeric(el)]\n",
    ").symmetric_difference(set(\n",
    "    [int(el) for el in df.ParticipantID.unique() if is_numeric(el)]\n",
    "))\n",
    "mask_series = df.participantId.map(lambda x: is_numeric(x))\n",
    "if mask_series.any():\n",
    "    df2 = df[mask_series]\n",
    "    mask_series = df2.participantId.map(lambda x: int(x) in id_set)\n",
    "    if mask_series.any(): display(df2[mask_series])\n",
    "mask_series = df.ParticipantID.map(lambda x: is_numeric(x))\n",
    "if mask_series.any():\n",
    "    df2 = df[mask_series]\n",
    "    mask_series = df2.ParticipantID.map(lambda x: int(x) in id_set)\n",
    "    if mask_series.any(): display(df2[mask_series])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e3b4cc0-cbaf-42fe-ae3a-61f712ed7538",
   "metadata": {},
   "source": [
    "\n",
    "1.\tCount of teleports (TeleportCount_sub; TeleportCount_jungle; TeleportCount_desert; TeleportCount_urban): teleport_count\n",
    "2.\tCount of voice captures: voice_capture_count\n",
    "3.\tCount of treatment applied (can you remove the pulse oximeter from this count?)\n",
    "4.\tCount of pulse oximeter use\n",
    "5.\tTotal distance covered in the scene: total_distance_covered\n",
    "6.\tAverage time between engage patient and either teleport or engage next patient\n",
    "7.\tAverage time per patient (by using any actions that take a patient as a property like gaze and treat and engage)\n",
    "8.\tCount of applied_tag\n",
    "9.\tApply_tag (expectant) 1 or 0\n",
    "10.\tCount of pulse taken\n",
    "pulse_oximeter_count   average_time_between_engagement_or_teleportation  average_time_per_patient                tag_applied_count           tag_applied_expectant   pulse_taken_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44291e69-325e-49a7-a90a-bb6c9dd3a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by sim 1 and 2\n",
    "from math import sqrt\n",
    "\n",
    "groupby_columns = fu.scene_groupby_columns + ['encounter_layout']\n",
    "rows_list = []\n",
    "for (session_uuid, scene_id, environment_id), scene_df in elevens_df.groupby(groupby_columns, dropna=False):\n",
    "    row_dict = {cn: eval(cn) for cn in fu.scene_groupby_columns + ['environment_id']}\n",
    "    row_dict['participant_id1'] = scene_df.participantId.squeeze()\n",
    "    row_dict['participant_id2'] = scene_df.ParticipantID.squeeze()\n",
    "    row_dict['sim_id1'] = scene_df.Sim1.squeeze()\n",
    "    row_dict['sim_id2'] = scene_df.Sim2.squeeze()\n",
    "    \n",
    "    for cn in participant_columns:\n",
    "        cv = scene_df[cn].mode().squeeze()\n",
    "        if not isinstance(cv, pd.Series): row_dict[cn] = cv\n",
    "\n",
    "    # Count of teleports (TeleportCount_sub; TeleportCount_jungle; TeleportCount_desert; TeleportCount_urban)\n",
    "    row_dict['teleport_count'] = fu.get_teleport_count(scene_df)\n",
    "    \n",
    "    # Count of voice captures\n",
    "    row_dict['voice_capture_count'] = fu.get_voice_capture_count(scene_df)\n",
    "\n",
    "    # Total distance covered in the scene\n",
    "    total_distance_covered = np.nan\n",
    "    mask_series = (scene_df.action_type == 'PLAYER_LOCATION') & ~scene_df.location_id.isnull()\n",
    "    if mask_series.any():\n",
    "        location_order = [eval(location_id) for location_id in scene_df[mask_series].sort_values('action_tick').location_id]\n",
    "        total_distance_covered = sum([\n",
    "            sqrt(\n",
    "                (first_tuple[0] - last_tuple[0])**2 + (first_tuple[1] - last_tuple[1])**2 + (first_tuple[2] - last_tuple[2])**2\n",
    "            ) for first_tuple, last_tuple in zip(location_order[:-1], location_order[1:])\n",
    "        ])\n",
    "    row_dict['total_distance_covered'] = total_distance_covered\n",
    "    \n",
    "    patients_list = eval(f'{environment_id.lower()}_patients_list')\n",
    "    mask_series = scene_df.patient_id.isin(patients_list)\n",
    "    participant_df = scene_df[mask_series]\n",
    "\n",
    "    # Count of treatment applied (can you remove the pulse oximeter from this count?)\n",
    "    mask_series = participant_df.tool_applied_type.isin([\n",
    "        'Gauze_Pack', 'Burn_Dressing', 'Gauze_Dressing', 'Pain_Meds', 'Needle', 'Naso', 'IV_Blood', 'SAM_Splint', 'Tourniquet', 'IV_Saline'\n",
    "    ])\n",
    "    row_dict['injury_treatments_count'] = participant_df[mask_series].shape[0]\n",
    "\n",
    "    # Count of pulse oximeter use\n",
    "    mask_series = participant_df.tool_applied_type.isin(['Pulse_Oximeter'])\n",
    "    row_dict['pulse_oximeter_count'] = participant_df[mask_series].shape[0]\n",
    "\n",
    "    # Average time between engage patient and either teleport or engage next patient\n",
    "    engagement_starts_list = []\n",
    "    teleport_starts_list = []\n",
    "    columns_list = ['patient_id', 'action_tick']\n",
    "\n",
    "    # Get the chronological order of engagement starts for each patient\n",
    "    for patient_id, patient_df in participant_df.groupby('patient_id'):\n",
    "        \n",
    "        # Check if the responder even interacted with this patient\n",
    "        mask_series = patient_df.action_type.isin(fu.responder_negotiations_list)\n",
    "        if mask_series.any():\n",
    "            df = patient_df[mask_series].sort_values('action_tick')\n",
    "            \n",
    "            # Get the first engagement start that has a location\n",
    "            mask_series = ~df.location_id.isnull()\n",
    "            if mask_series.any(): engagement_start = df[mask_series].iloc[0].action_tick\n",
    "            else: engagement_start = df.iloc[0].action_tick\n",
    "            \n",
    "            # Add engagement information to the list\n",
    "            engagement_tuple = (patient_id, engagement_start)\n",
    "            engagement_starts_list.append(engagement_tuple)\n",
    "    \n",
    "    # Sort the starts list chronologically\n",
    "    if engagement_starts_list:\n",
    "        engagement_starts_df = DataFrame(sorted(engagement_starts_list, key=lambda x: x[1], reverse=False), columns=columns_list)\n",
    "\n",
    "    # Get the teleport starts\n",
    "    mask_series = ~participant_df.teleport_location.isnull()\n",
    "    if mask_series.any(): teleport_starts_list = [('teleport', action_tick) for action_tick in participant_df[mask_series].action_tick]\n",
    "    \n",
    "    # Sort the starts list chronologically\n",
    "    if teleport_starts_list:\n",
    "        teleport_starts_df = DataFrame(sorted(teleport_starts_list, key=lambda x: x[1], reverse=False), columns=columns_list)\n",
    "    \n",
    "    # Merge the two data frames together\n",
    "    if engagement_starts_list and teleport_starts_list:\n",
    "        df = pd.concat([engagement_starts_df, teleport_starts_df], axis='index').sort_values('action_tick').reset_index(drop=True)\n",
    "        \n",
    "        # Remove all but the last sequential teleport\n",
    "        previous_teleport = False\n",
    "        last_patient = ''\n",
    "        last_index = -1\n",
    "        drop_list = []\n",
    "        for row_index, patient_id in df.patient_id.iteritems():\n",
    "            if all(map(lambda x: x == 'teleport', [last_patient, patient_id])): previous_teleport = True\n",
    "            if previous_teleport: drop_list.append(last_index)\n",
    "            previous_teleport = False\n",
    "            last_patient = patient_id\n",
    "            last_index = row_index\n",
    "        df = df.drop(index=drop_list)\n",
    "    \n",
    "    elif engagement_starts_list: df = engagement_starts_df\n",
    "    elif teleport_starts_list: df = teleport_starts_df\n",
    "    event_order = df.sort_values('action_tick').action_tick.tolist()\n",
    "    average_time_elapsed = np.mean(np.array([\n",
    "        abs(first_event - last_event) for first_event, last_event in zip(event_order[:-1], event_order[1:])\n",
    "    ]))\n",
    "    row_dict['average_time_between_engagement_or_teleportation'] = average_time_elapsed\n",
    "\n",
    "    # Average time per patient (by using any actions that take a patient as a property like gaze and treat and engage)\n",
    "    gaze_starts_list = []\n",
    "\n",
    "    # Get the gaze starts\n",
    "    mask_series = ~participant_df.player_gaze_location.isnull()\n",
    "    if mask_series.any(): gaze_starts_list = [(patient_id, action_tick) for (patient_id, action_tick), _ in participant_df[mask_series].groupby(\n",
    "        ['patient_id', 'action_tick']\n",
    "    )]\n",
    "    \n",
    "    # Sort the starts list chronologically\n",
    "    if gaze_starts_list:\n",
    "        gaze_starts_df = DataFrame(sorted(gaze_starts_list, key=lambda x: x[1], reverse=False), columns=columns_list)\n",
    "    \n",
    "    # Merge the two data frames together\n",
    "    if engagement_starts_list and gaze_starts_list:\n",
    "        df = pd.concat([engagement_starts_df, gaze_starts_df], axis='index').sort_values('action_tick').reset_index(drop=True)\n",
    "        \n",
    "        # Remove all but the last consecutive patient ID\n",
    "        previous_gaze = False\n",
    "        previous_patient = ''\n",
    "        previous_index = -1\n",
    "        drop_list = []\n",
    "        for row_index, patient_id in df.patient_id.iteritems():\n",
    "            if previous_patient == patient_id: previous_gaze = True\n",
    "            if previous_gaze: drop_list.append(previous_index)\n",
    "            previous_gaze = False\n",
    "            previous_patient = patient_id\n",
    "            previous_index = row_index\n",
    "        df = df.drop(index=drop_list)\n",
    "    \n",
    "    elif engagement_starts_list: df = engagement_starts_df\n",
    "    event_order = df.sort_values('action_tick').action_tick.tolist()\n",
    "    average_time_elapsed = sum([\n",
    "        abs(first_event - last_event) for first_event, last_event in zip(event_order[:-1], event_order[1:])\n",
    "    ]) / fu.get_patient_count(participant_df)\n",
    "    row_dict['average_time_per_patient'] = average_time_elapsed\n",
    "    \n",
    "    # Count of applied_tag\n",
    "    mask_series = participant_df.action_type.isin(['TAG_APPLIED'])\n",
    "    row_dict['tag_applied_count'] = participant_df[mask_series].shape[0]\n",
    "    \n",
    "    # Apply_tag (expectant) 1 or 0\n",
    "    mask_series = participant_df.patient_salt.isin(['EXPECTANT'])\n",
    "    patient_ids_list = participant_df[mask_series].patient_id.unique().tolist()\n",
    "    mask_series = participant_df.patient_id.isin(patient_ids_list)\n",
    "    tag_values_list = [fu.get_tag_value(patient_df) for _, patient_df in participant_df[mask_series].groupby('patient_id')]\n",
    "    assert len(tag_values_list) == 1, \"You have more than one EXPECTANT patient\"\n",
    "    row_dict['tag_applied_expectant'] = tag_values_list[0]\n",
    "    \n",
    "    # Count of pulse taken\n",
    "    row_dict['pulse_taken_count'] = fu.get_pulse_taken_count(participant_df)\n",
    "\n",
    "    rows_list.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a10c0e4e-1db1-4faf-8135-4274fd4522c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 40)\n",
      "Pickling to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/metrics_evaluation_open_world_participants_df.pkl\n",
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/metrics_evaluation_open_world_participants_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = DataFrame(rows_list)\n",
    "print(df.shape)\n",
    "nu.store_objects(metrics_evaluation_open_world_participants_df=df)\n",
    "nu.save_data_frames(metrics_evaluation_open_world_participants_df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f4aa9105-1838-4845-b371-aa9f6ad1a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 44)\n",
      "Pickling to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/pkl/metrics_evaluation_open_world_participants_df.pkl\n",
      "Saving to /mnt/c/Users/DaveBabbitt/Documents/GitHub/itm-analysis-reporting/saves/csv/metrics_evaluation_open_world_participants_df.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_list = ['environment_id', 'teleport_count']\n",
    "\n",
    "# Use pivot_table with fill_value for missing entries\n",
    "pivoted_df = df[columns_list].pivot_table(index=df.index, columns='environment_id', values='teleport_count', fill_value=np.nan)\n",
    "\n",
    "# Rename columns to remove spaces (optional)\n",
    "pivoted_df.columns = ['teleport_count_' + col.lower().replace(' ', '_') for col in pivoted_df.columns]\n",
    "\n",
    "combined_df = pd.concat([df, pivoted_df], axis='columns').reset_index(drop=True)\n",
    "print(combined_df.shape)\n",
    "nu.store_objects(metrics_evaluation_open_world_participants_df=combined_df)\n",
    "nu.save_data_frames(metrics_evaluation_open_world_participants_df=combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f5865c3-ad43-4c56-8750-b0b35b0d552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'environment_id': ['Submarine', 'Urban', 'Urban', 'Urban', 'Urban', 'Submarine', 'Desert', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Urban', 'Jungle', 'Submarine', 'Jungle', 'Jungle', 'Urban', 'Jungle', 'Desert', 'Jungle', 'Desert', 'Jungle', 'Submarine', 'Desert', 'Desert', 'Submarine', 'Desert', 'Jungle', 'Jungle', 'Urban', 'Urban', 'Submarine'], 'teleport_count': [62, 0, 2, 35, 87, 109, 6, 27, 17, 33, 7, 75, 35, 80, 114, 71, 32, 82, 63, 16, 0, 26, 53, 74, 38, 77, 55, 65, 0, 68, 126, 21]}\n",
      "\n",
      "        from pandas import DataFrame\n",
      "        \n",
      "        df = DataFrame({\n",
      "            \"environment_id\": ['Submarine', 'Urban', 'Urban', 'Urban', 'Urban', 'Submarine', 'Desert', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Urban', 'Jungle', 'Submarine', 'Jungle', 'Jungle', 'Urban', 'Jungle', 'Desert', 'Jungle', 'Desert', 'Jungle', 'Submarine', 'Desert', 'Desert', 'Submarine', 'Desert', 'Jungle', 'Jungle', 'Urban', 'Urban', 'Submarine'],\n",
      "            \"teleport_count\": [62, 0, 2, 35, 87, 109, 6, 27, 17, 33, 7, 75, 35, 80, 114, 71, 32, 82, 63, 16, 0, 26, 53, 74, 38, 77, 55, 65, 0, 68, 126, 21]\n",
      "        })\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[columns_list].to_dict(orient='list'))\n",
    "print(f\"\"\"\n",
    "        from pandas import DataFrame\n",
    "        \n",
    "        df = DataFrame({{\n",
    "            \"environment_id\": {df.environment_id.tolist()},\n",
    "            \"teleport_count\": {df.teleport_count.tolist()}\n",
    "        }})\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a112fcd-9fcc-48d6-a065-b322c59911c8",
   "metadata": {},
   "source": [
    "\n",
    "# Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b55797-2e72-48f1-84ec-3304678868f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Test an example regression equation\n",
    "import unittest\n",
    "\n",
    "suite = unittest.TestSuite()\n",
    "class RegressionEquationTest(unittest.TestCase):\n",
    "    def test_is_very_close_to_zero(self):\n",
    "        result = (anova_df.mean_percent_accurate_tagging - (\n",
    "            11.5687 +\n",
    "            5.0980 * anova_df.Environment_Desert +\n",
    "            3.3303 * anova_df.Environment_Jungle +\n",
    "            2.0424 * anova_df.Environment_Submarine +\n",
    "            1.0980 * anova_df.Environment_Urban\n",
    "        )).sum()\n",
    "        self.assertAlmostEqual(result, 0, places=2)\n",
    "suite.addTest(unittest.makeSuite(RegressionEquationTest))\n",
    "\n",
    "# Run the test suite\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5d60f80-e24b-4ee9-86d6-69643ddf5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for specific inconsistency with the JSON data\n",
    "mask_series = (elevens_df.configData_scene == 'sim-desert') & (elevens_df.configData_scenarioData_name == 'Jungle')\n",
    "if mask_series.any(): display(elevens_df[mask_series].dropna(axis='columns', how='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b622cba-c51f-4070-8c8f-8b7112bcf9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sim-jungle', 'sim-sub', 'sim-desert', 'sim-urban-sanitized', nan]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show all sim types\n",
    "elevens_df.configData_scene.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a645ab37-c27c-46f5-a676-e166d07a922e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jungle Eval', 'Submarine Eval', 'Desert', 'Urban', nan]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show all sim names\n",
    "elevens_df.configData_scenarioData_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d279a62-a5dc-4b1c-97b3-24b831a43947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configData_scene</th>\n",
       "      <th>configData_scenarioData_name</th>\n",
       "      <th>configData_scenarioData_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48886</th>\n",
       "      <td>sim-desert</td>\n",
       "      <td>Desert</td>\n",
       "      <td>Desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>sim-jungle</td>\n",
       "      <td>Jungle Eval</td>\n",
       "      <td>Jungle Eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>sim-sub</td>\n",
       "      <td>Submarine Eval</td>\n",
       "      <td>submarine Eval Scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60314</th>\n",
       "      <td>sim-urban-sanitized</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban Scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81568</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          configData_scene configData_scenarioData_name  \\\n",
       "48886           sim-desert                       Desert   \n",
       "1526            sim-jungle                  Jungle Eval   \n",
       "6451               sim-sub               Submarine Eval   \n",
       "60314  sim-urban-sanitized                        Urban   \n",
       "81568                  NaN                          NaN   \n",
       "\n",
       "      configData_scenarioData_description  \n",
       "48886                              Desert  \n",
       "1526                          Jungle Eval  \n",
       "6451              submarine Eval Scenario  \n",
       "60314                      Urban Scenario  \n",
       "81568                                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>record_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>configData_scene</th>\n",
       "      <th>configData_scenarioData_name</th>\n",
       "      <th>configData_scenarioData_description</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sim-desert</th>\n",
       "      <th>Desert</th>\n",
       "      <th>Desert</th>\n",
       "      <td>14936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim-jungle</th>\n",
       "      <th>Jungle Eval</th>\n",
       "      <th>Jungle Eval</th>\n",
       "      <td>20634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim-sub</th>\n",
       "      <th>Submarine Eval</th>\n",
       "      <th>submarine Eval Scenario</th>\n",
       "      <td>18519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim-urban-sanitized</th>\n",
       "      <th>Urban</th>\n",
       "      <th>Urban Scenario</th>\n",
       "      <td>16657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>9618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      record_count\n",
       "configData_scene    configData_scenarioData_name configData_scenarioData_description              \n",
       "sim-desert          Desert                       Desert                                      14936\n",
       "sim-jungle          Jungle Eval                  Jungle Eval                                 20634\n",
       "sim-sub             Submarine Eval               submarine Eval Scenario                     18519\n",
       "sim-urban-sanitized Urban                        Urban Scenario                              16657\n",
       "NaN                 NaN                          NaN                                          9618"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Show all sim combos\n",
    "columns_list = ['configData_scene', 'configData_scenarioData_name', 'configData_scenarioData_description']\n",
    "display(elevens_df[columns_list].drop_duplicates().sort_values(columns_list))\n",
    "display(elevens_df.groupby(columns_list, dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23a954b-f307-4538-a3e0-c0a93748d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sim-desert\n",
      "(173, 114)\n",
      "6db9446c-2cd4-41b4-be8d-be5ccbbc6e05 0\n",
      "Has the patients for the desert environment\n",
      "7e23cc31-422a-42e1-acb5-964c661750f4 0\n",
      "Has the patients for the desert environment\n",
      "b5989edc-8348-4b84-b649-87fc4f1cca53 0\n",
      "Has the patients for the desert environment\n",
      "bccb0095-5efd-4c5c-ad58-8b8624f9ab56 0\n",
      "Has the patients for the desert environment\n",
      "c99de80f-15cc-45cb-aa64-5af0f2f118ca 0\n",
      "Has the patients for the desert environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sim-jungle\n",
      "(506, 114)\n",
      "287389c4-4c48-4483-87c0-6b363b57bde2 0\n",
      "Has the patients for the jungle environment\n",
      "2e8f6555-a7fa-4b54-8132-c030d697b4ad 0\n",
      "Has the patients for the jungle environment\n",
      "3cf14e31-f416-4c78-8a69-91bf0c685448 0\n",
      "Has the patients for the jungle environment\n",
      "44484bce-c7cc-41ca-a871-f7b9b2e3c847 0\n",
      "Has the patients for the jungle environment\n",
      "4bc46c8c-66e7-463d-b3a1-2a8303af4fd1 0\n",
      "Has the patients for the jungle environment\n",
      "5d94f0d4-a1b1-4d18-8a62-591e196006a9 0\n",
      "Has the patients for the jungle environment\n",
      "67dc0230-511d-41ac-ae9b-850900ab9e6a 0\n",
      "Has the patients for the jungle environment\n",
      "70eef02d-d2d0-458e-a8bb-f6511bf47a0c 0\n",
      "Has the patients for the jungle environment\n",
      "8839e3b8-be5e-4878-8aaf-26c656ae2270 0\n",
      "Has the patients for the jungle environment\n",
      "d13091cc-98e4-4aba-8d02-7eca8bd1a30c 0\n",
      "Has the patients for the jungle environment\n",
      "de6d297c-23d6-4f85-a873-f48e90b01542 0\n",
      "Has the patients for the jungle environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sim-sub\n",
      "(389, 114)\n",
      "04f80090-9e61-431d-8473-dccb75fed04d 0\n",
      "Has the patients for the submarine environment\n",
      "21f8cb5d-f5ac-4a01-9287-43df5f6751a1 0\n",
      "Has the patients for the submarine environment\n",
      "50b15e40-9860-4574-8ab8-0bd960fe27de 0\n",
      "Has the patients for the submarine environment\n",
      "a7ce6f7b-6466-4281-9496-92b640d9d04b 0\n",
      "Has the patients for the submarine environment\n",
      "c6d3a90f-68c0-4948-bd96-537e80973605 0\n",
      "Has the patients for the submarine environment\n",
      "e8b9f065-d449-4dee-98e7-298568054411 0\n",
      "Has the patients for the submarine environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sim-urban-sanitized\n",
      "(269, 114)\n",
      "0c0cc880-b3fb-488d-a468-0e67c17ca176 0\n",
      "Has the patients for the urban environment\n",
      "0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492 0\n",
      "Has the patients for the urban environment\n",
      "12988238-24b6-4ac2-9f33-398321e82ae0 0\n",
      "Has the patients for the urban environment\n",
      "1995e7ef-ef02-4fc1-b1ab-f137dbf69d48 0\n",
      "Has the patients for the urban environment\n",
      "23081f6e-875e-44f5-8bd0-edc3905f5c2c 1\n",
      "Has the patients for the desert environment\n",
      "45365e18-6e38-48e7-b4a2-6b448b209034 0\n",
      "Has the patients for the urban environment\n",
      "6666ee6b-863b-49d1-8097-97e6aa4fb39d 0\n",
      "Has the patients for the urban environment\n",
      "df2fcf88-874b-4cf9-9707-3fa0b30c348f 0\n",
      "Has the patients for the urban environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16608, 114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "(443, 114)\n",
      "5d8d73a3-1898-4f64-8676-73edd1b7daa0 0\n",
      "Has the patients for the jungle environment\n",
      "dfec642f-45c9-4813-91d8-3445d5ca763c 0\n",
      "Has the patients for the urban environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5656, 114)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Explicitly display all inconsistencies within the JSON data\n",
    "print()\n",
    "columns_list = ['configData_scene', 'configData_scenarioData_name', 'configData_scenarioData_description']\n",
    "desert_patients_list = [\n",
    "    'Open World Marine 1 Female Root', 'Open World Marine 2 Male Root', 'Open World Civilian 1 Male Root', 'Open World Civilian 2 Female Root'\n",
    "]\n",
    "jungle_patients_list = ['Open World Marine 1 Male Root', 'Open World Marine 2 Female Root', 'Open World Marine 3 Male Root', 'Open World Marine 4 Male Root']\n",
    "submarine_patients_list = ['Navy Soldier 1 Male Root', 'Navy Soldier 2 Male Root', 'Navy Soldier 3 Male Root', 'Navy Soldier 4 Female Root']\n",
    "urban_patients_list = ['Marine 1 Male Root', 'Marine 2 Male Root', 'Marine 3 Male Root', 'Marine 4 Male Root', 'Civilian 1 Female Root']\n",
    "for (configData_scene, configData_scenarioData_name, configData_scenarioData_description), environment_df in elevens_df.groupby(columns_list, dropna=False):\n",
    "    print(); print(configData_scene)\n",
    "    if configData_scene == 'sim-desert':\n",
    "        patients_list = desert_patients_list\n",
    "    elif configData_scene == 'sim-jungle':\n",
    "        patients_list = jungle_patients_list\n",
    "    elif configData_scene == 'sim-sub':\n",
    "        patients_list = submarine_patients_list\n",
    "    elif configData_scene == 'sim-urban-sanitized':\n",
    "        patients_list = urban_patients_list\n",
    "    else:\n",
    "        patients_list = desert_patients_list + jungle_patients_list + submarine_patients_list + urban_patients_list\n",
    "    mask_series = environment_df.patient_id.isin(patients_list)\n",
    "    print(environment_df[mask_series].shape)\n",
    "    for (session_uuid, scene_id), scene_df in environment_df.groupby(fu.scene_groupby_columns):\n",
    "        print(session_uuid, scene_id)\n",
    "        for env_str in ['desert', 'jungle', 'submarine', 'urban']:\n",
    "            patients_list = eval(f'{env_str}_patients_list')\n",
    "            if all(map(lambda patient_id: patient_id in scene_df.patient_id.unique().tolist(), patients_list)):\n",
    "                print(f'Has the patients for the {env_str} environment')\n",
    "    #     for patient_id in patients_list: print(patient_id, patient_id in scene_df.patient_id.unique().tolist())\n",
    "    gb = environment_df.groupby(fu.scene_groupby_columns).filter(\n",
    "        lambda scene_df: all(map(lambda patient_id: patient_id in scene_df.patient_id.unique().tolist(), patients_list))\n",
    "    )\n",
    "    display(gb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ca1e83e-ef7f-43d5-b99c-7394e2640a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        from pandas import DataFrame\n",
      "        \n",
      "        anova_df = DataFrame({\n",
      "            'Environment': ['Desert', 'Desert', 'Desert', 'Desert', 'Desert', 'Desert', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Jungle', 'Submarine', 'Submarine', 'Submarine', 'Submarine', 'Submarine', 'Submarine', 'Urban', 'Urban', 'Urban', 'Urban', 'Urban', 'Urban', 'Urban', 'Urban'],\n",
      "            'session_uuid': ['23081f6e-875e-44f5-8bd0-edc3905f5c2c', '6db9446c-2cd4-41b4-be8d-be5ccbbc6e05', '7e23cc31-422a-42e1-acb5-964c661750f4', 'b5989edc-8348-4b84-b649-87fc4f1cca53', 'bccb0095-5efd-4c5c-ad58-8b8624f9ab56', 'c99de80f-15cc-45cb-aa64-5af0f2f118ca', '287389c4-4c48-4483-87c0-6b363b57bde2', '2e8f6555-a7fa-4b54-8132-c030d697b4ad', '3cf14e31-f416-4c78-8a69-91bf0c685448', '44484bce-c7cc-41ca-a871-f7b9b2e3c847', '4bc46c8c-66e7-463d-b3a1-2a8303af4fd1', '5d8d73a3-1898-4f64-8676-73edd1b7daa0', '5d94f0d4-a1b1-4d18-8a62-591e196006a9', '67dc0230-511d-41ac-ae9b-850900ab9e6a', '70eef02d-d2d0-458e-a8bb-f6511bf47a0c', '8839e3b8-be5e-4878-8aaf-26c656ae2270', 'd13091cc-98e4-4aba-8d02-7eca8bd1a30c', 'de6d297c-23d6-4f85-a873-f48e90b01542', '04f80090-9e61-431d-8473-dccb75fed04d', '21f8cb5d-f5ac-4a01-9287-43df5f6751a1', '50b15e40-9860-4574-8ab8-0bd960fe27de', 'a7ce6f7b-6466-4281-9496-92b640d9d04b', 'c6d3a90f-68c0-4948-bd96-537e80973605', 'e8b9f065-d449-4dee-98e7-298568054411', '0c0cc880-b3fb-488d-a468-0e67c17ca176', '0fb9b0c5-21bb-4f2a-b97f-a01a5e67e492', '12988238-24b6-4ac2-9f33-398321e82ae0', '1995e7ef-ef02-4fc1-b1ab-f137dbf69d48', '45365e18-6e38-48e7-b4a2-6b448b209034', '6666ee6b-863b-49d1-8097-97e6aa4fb39d', 'df2fcf88-874b-4cf9-9707-3fa0b30c348f', 'dfec642f-45c9-4813-91d8-3445d5ca763c'],\n",
      "            'scene_id': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "            'mean_first_patient_interaction': [nan, 310118.0, nan, 445584.0, 410555.0, 478277.36363636365, 353250.0, 416781.8, 386460.8888888889, 213175.0, 263152.1666666667, 329643.5, 284320.1818181818, 302604.1666666667, 445027.0, 394912.9, 232868.7, nan, 517025.8, 433719.1111111111, 332083.8, 387588.0, 457897.7272727273, 270059.0, nan, 114906.0, 408641.0, 450979.2727272727, 381027.78571428574, 264569.875, 456568.1, 400179.28571428574],\n",
      "            'mean_last_patient_interaction': [nan, 360530.5714285714, nan, 494529.1818181818, 438177.8, 592810.2727272727, 420412.125, 490330.2, 456381.8888888889, 243815.0, 361486.1666666667, 456248.125, 389784.7272727273, 355021.0833333333, 572533.5454545454, 486827.8, 346313.9, nan, 631129.8, 535067.0, 393426.6, 505269.0, 576238.7272727273, 444127.3333333333, nan, 114906.0, 532080.6666666666, 539555.5454545454, 535622.6428571428, 359329.0, 522651.8, 516954.5],\n",
      "            'total_patient_engagement_count': [0, 7, 0, 11, 10, 14, 13, 15, 14, 3, 15, 27, 25, 13, 27, 17, 45, 0, 12, 13, 11, 17, 13, 3, 0, 0, 3, 24, 33, 24, 19, 16],\n",
      "            'total_action_count': [0, 39, 0, 71, 49, 77, 61, 29, 73, 26, 61, 92, 88, 105, 80, 94, 115, 0, 101, 88, 68, 106, 110, 20, 0, 1, 31, 90, 160, 73, 72, 95],\n",
      "            'total_assessment_count': [0, 19, 0, 33, 17, 32, 21, 21, 35, 5, 44, 45, 53, 58, 46, 57, 76, 0, 31, 42, 19, 36, 45, 11, 0, 0, 19, 69, 94, 39, 34, 28],\n",
      "            'total_treatment_count': [0, 7, 0, 15, 15, 14, 18, 3, 13, 5, 5, 18, 13, 15, 11, 15, 14, 0, 27, 17, 19, 26, 25, 2, 0, 0, 3, 6, 24, 8, 14, 23],\n",
      "            'total_tag_application_count': [0, 7, 0, 8, 5, 9, 5, 2, 6, 0, 5, 6, 5, 9, 5, 8, 5, 0, 11, 9, 9, 11, 8, 2, 0, 0, 3, 5, 11, 8, 7, 11],\n",
      "            'max_time_to_hemorrhage_control': [0, 9454, 0, 17564, 12052, 211280, 881, 0, 65044, 0, 0, 109308, 2216, 28394, 0, 65651, 110854, 0, 29924, 20111, 46209, 65866, 40554, 0, 0, 0, 106864, 171059, 201095, 80776, 47665, 29175],\n",
      "            'treated_expectant_count': [0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 2],\n",
      "            'total_time_to_triage_scene': [2250, 1057, 855, 789688, 579884, 822997, 572999, 611389, 679682, 11210, 1196, 745177, 575949, 604246, 910933, 877452, 596815, 2942, 917557, 744224, 559210, 730139, 715636, 12420, 1154, 1007, 1265, 740836, 869251, 1035, 794357, 874562],\n",
      "            'time_to_hemorrhage_control': [2250, 1056, 855, 609795, 544783, 803935, 572240, 4726, 585199, 11210, 1195, 567261, 2216, 452916, 1697, 728559, 391444, 2942, 878330, 724787, 498053, 604000, 530618, 12420, 1154, 1007, 1265, 9735, 724386, 1035, 790829, 797221],\n",
      "            'mean_time_for_hemorrhage_control_per_patient': [0.0, 0.0, 0.0, 6424.25, 7175.5, 61864.0, 440.5, 0.0, 32522.0, 0.0, 0.0, 54654.0, 1108.0, 14197.0, 0.0, 32825.5, 55427.0, 0.0, 15115.333333333334, 6703.666666666667, 19695.333333333332, 27823.666666666668, 16905.666666666668, 0.0, 0.0, 0.0, 0.0, 0.0, 48456.8, 0.0, 18259.2, 10881.0],\n",
      "            'mean_percent_accurate_tagging': [0.0, 0.0, 0.0, 44.285714285714285, 32.857142857142854, 22.857142857142858, 13.636363636363637, 13.636363636363637, 28.939393939393945, 0.0, 0.0, 13.636363636363637, 22.121212121212125, 28.939393939393945, 22.121212121212125, 20.454545454545457, 15.303030303030303, 0.0, 21.666666666666668, 0.0, 27.5, 32.5, 0.0, 0.0, 0.0, 0.0, 4.666666666666667, 11.333333333333332, 30.0, 4.666666666666667, 20.666666666666668, 30.0],\n",
      "        })\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display a manually created data frame for testing and prompt generation\n",
    "print('''\n",
    "        from pandas import DataFrame\n",
    "        \n",
    "        anova_df = DataFrame({''')\n",
    "for k, v in anova_df.to_dict(orient='list').items(): print(f\"            '{k}': {v},\")\n",
    "print('''        })''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e2081-5d1a-40dc-b9f4-eec5d6724d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
