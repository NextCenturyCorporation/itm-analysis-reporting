{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8bde5c-3e38-44de-b3f8-d69d2e0a55f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up notebook\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef2c267-3068-44f9-9b93-60e6a43c1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "from FRVRS import nu, fu\n",
    "from numpy import nan, isnan\n",
    "from os import listdir as listdir, makedirs as makedirs, path as osp, remove as remove, sep as sep, walk as walk\n",
    "from pandas import (\n",
    "    CategoricalDtype, DataFrame, Index, NaT, Series, concat, get_dummies, isna, notnull, read_csv, read_excel, to_datetime, to_numeric\n",
    ")\n",
    "from re import split, search, sub, MULTILINE\n",
    "from scipy.stats import f_oneway, ttest_ind, kruskal, norm\n",
    "import itertools\n",
    "import re\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bff7d3-e97b-4520-b402-aa026fbbfb45",
   "metadata": {},
   "source": [
    "\n",
    "## Use the CSV Data from the CACI Logs (Human_Sim_Metrics_Data_4-12-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade404a-dd79-4ed7-9df1-c6c237a1157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get all the Open World logs into one data frame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In the zip there are 51 folders, (51 JSON, 51 CSV).\n",
    "# All the files are named appropriated in the folder/csv/json UUID_ParticipantID.\n",
    "# Some of the internal Participants IDs might be off because the moderator forgot to enter a Participant ID or didn't enter\n",
    "# the Participant ID correctly so we needed to figure out which participant it was.\n",
    "# So only utilize the UUID and Participant ID that is on the file name to identify and ignore the internal Participant IDs.\n",
    "print(\"\\nGet all the Open World logs into one data frame\")\n",
    "csv_stats_df = DataFrame([])\n",
    "logs_path = osp.join(nu.data_folder, 'logs', 'Human_Sim_Metrics_Data_4-12-2024')\n",
    "directories_list = listdir(logs_path)\n",
    "for dir_name in directories_list:\n",
    "    \n",
    "    # Add the CSVs to the data frame\n",
    "    folder_path = osp.join(logs_path, dir_name)\n",
    "    df = fu.concatonate_logs(logs_folder=folder_path)\n",
    "    \n",
    "    session_uuid, participant_id = dir_name.split('_')\n",
    "    df['session_uuid'] = session_uuid\n",
    "    df['participant_id'] = int(participant_id)\n",
    "    \n",
    "    # Remove numerically-named columns\n",
    "    columns_list = [x for x in df.columns if not search(r'\\d+', str(x))]\n",
    "    df = df[columns_list]\n",
    "    \n",
    "    # Convert 'TRUE' and 'FALSE' to boolean values\n",
    "    for cn in fu.boolean_columns_list:\n",
    "        df[cn] = df[cn].map({'TRUE': True, 'FALSE': False, 'True': True, 'False': False})\n",
    "    \n",
    "    # Convert the nulls into NaNs\n",
    "    for cn in df.columns: df[cn] = df[cn].replace(['null', 'nan', 'n'], nan)\n",
    "    \n",
    "    # Append the data frame for the current subdirectory to the main data frame and break the participant ID loop\n",
    "    csv_stats_df = concat([csv_stats_df, df], axis='index')\n",
    "\n",
    "csv_stats_df = csv_stats_df.reset_index(drop=True).drop_duplicates()\n",
    "csv_stats_df['csv_file_name'] = csv_stats_df.csv_file_subpath.map(lambda x: str(x).split('/')[-1])\n",
    "\n",
    "# Check for proper ingestion (duplicate file ingestion, et al)\n",
    "assert len(csv_stats_df.columns) > 4, \"Nothing ingested\"\n",
    "assert csv_stats_df.participant_id.nunique() == 26, f\"Participant count should be 26, it's {csv_stats_df.participant_id.nunique()} instead\"\n",
    "\n",
    "# Check that all the rows that have more than one unique value in the file_name column for each value in the session_uuid column\n",
    "mask_series = (csv_stats_df.groupby('session_uuid').csv_file_subpath.transform(Series.nunique) > 1)\n",
    "assert not mask_series.any(), \"You have duplicate files\"\n",
    "\n",
    "print(csv_stats_df.shape)\n",
    "print(csv_stats_df.groupby('participant_id').size().to_frame().rename(columns={0: 'record_count'}))\n",
    "display(csv_stats_df.sample(4).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84c0c8-f9da-497c-8bfd-41a7ab7e4f8b",
   "metadata": {},
   "source": [
    "\n",
    "### Truncate the CSV data to only include our patients at the times they were engaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc28764-ff60-4224-9ffe-1d121d49b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a dataset of each CSV file and the action tick where CACI patients first appear\n",
    "desert_patients_list = [\n",
    "    'Open World Marine 1 Female Root', 'Open World Marine 2 Male Root', 'Open World Civilian 1 Male Root', 'Open World Civilian 2 Female Root'\n",
    "]\n",
    "jungle_patients_list = [\n",
    "    'Open World Marine 1 Male Root', 'Open World Marine 2 Female Root', 'Open World Marine 3 Male Root', 'Open World Marine 4 Male Root'\n",
    "]\n",
    "submarine_patients_list = ['Navy Soldier 1 Male Root', 'Navy Soldier 2 Male Root', 'Navy Soldier 3 Male Root', 'Navy Soldier 4 Female Root']\n",
    "urban_patients_list = ['Marine 1 Male Root', 'Marine 2 Male Root', 'Marine 3 Male Root', 'Marine 4 Male Root', 'Civilian 1 Female Root']\n",
    "patients_set = set(desert_patients_list + jungle_patients_list + submarine_patients_list + urban_patients_list)\n",
    "mask_series = ~csv_stats_df.patient_id.isnull()\n",
    "assert patients_set.issubset(set(csv_stats_df[mask_series].patient_id)), \"Our patients lists are not in the CSVs\"\n",
    "mask_series = csv_stats_df.patient_id.isin(patients_set)\n",
    "rows_list = []\n",
    "for csv_file_name, csv_file_name_df in csv_stats_df[mask_series].groupby('csv_file_name'):\n",
    "    row_dict = {'csv_file_name': csv_file_name}\n",
    "    action_tick = csv_file_name_df.action_tick.min()\n",
    "    row_dict['first_occurence'] = action_tick\n",
    "    rows_list.append(row_dict)\n",
    "first_occurence_df = DataFrame(rows_list)\n",
    "print(first_occurence_df.shape)\n",
    "display(first_occurence_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ede0d-11f1-4837-aeb4-07010d6d4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From each file, filter out the time before the first appearance of the CACI patients\n",
    "truncated_df = DataFrame([])\n",
    "for (csv_file_name, first_occurence), _ in first_occurence_df.groupby(['csv_file_name', 'first_occurence']):\n",
    "    mask_series = (csv_stats_df.csv_file_name == csv_file_name) & (csv_stats_df.action_tick >= first_occurence)\n",
    "    df = csv_stats_df[mask_series]\n",
    "    truncated_df = concat([truncated_df, df], axis='index')\n",
    "print(truncated_df.shape)\n",
    "display(truncated_df.sample(4).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19feeff-527a-450a-aa9c-4be7f3ec0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of patients that CACI doesn't care about\n",
    "mask_series = ~csv_stats_df.patient_id.isin(patients_set) & ~csv_stats_df.patient_id.isnull()\n",
    "non_patients_list = csv_stats_df[mask_series].patient_id.unique().tolist()\n",
    "non_patients_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcc034-4528-4651-b14e-0b7060ebb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out just the patients that CACI doesn't care about\n",
    "mask_series = ~truncated_df.patient_id.isin(non_patients_list)\n",
    "truncated_df = truncated_df[mask_series]\n",
    "print(truncated_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc9293-88e7-4bfa-97e7-7a09a0ad1712",
   "metadata": {},
   "source": [
    "\n",
    "## Create the Scene Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089bba6-c905-4706-920b-5de4e5c88a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the scene data frame precursor\n",
    "distance_delta_df = fu.get_distance_deltas_data_frame(truncated_df)\n",
    "print(distance_delta_df.shape)\n",
    "display(distance_delta_df.sample(5).T)\n",
    "display(distance_delta_df.groupby('patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedeb2c-bafe-4ef3-8132-26a149701ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the scene stats data frame\n",
    "rows_list = []\n",
    "for (session_uuid, scene_id), idx_df in distance_delta_df.groupby(fu.scene_groupby_columns):\n",
    "    row_dict = list(idx_df.T.to_dict().values())[0]\n",
    "    \n",
    "    # mean_patient_count\n",
    "    patient_count = row_dict.pop('patient_count')\n",
    "    row_dict.update({'scene_patient_count': patient_count})\n",
    "    \n",
    "    # Get the whole scene history\n",
    "    mask_series = True\n",
    "    for cn in fu.scene_groupby_columns: mask_series &= (truncated_df[cn] == eval(cn))\n",
    "    scene_df = truncated_df[mask_series]\n",
    "    \n",
    "    if scene_df.shape[0]:\n",
    "        row_dict['participant_id'] = scene_df.participant_id.iloc[0]\n",
    "        \n",
    "        # Get the count of all the patient injuries\n",
    "        all_patient_injuries_count = 0\n",
    "        for patient_id, patient_df in scene_df.groupby('patient_id'):\n",
    "            all_patient_injuries_count += patient_df.injury_id.nunique()\n",
    "        row_dict['scene_patient_injuries_count'] = all_patient_injuries_count\n",
    "        \n",
    "        # percent_injury_correctly_treated\n",
    "        correctly_treated_count = fu.get_injury_correctly_treated_count(scene_df)\n",
    "        row_dict['scene_correctly_treated_count'] = correctly_treated_count\n",
    "        try: percent_injury_correctly_treated = 100 * correctly_treated_count / all_patient_injuries_count\n",
    "        except ZeroDivisionError: percent_injury_correctly_treated = nan\n",
    "        row_dict['scene_percent_injury_correctly_treated'] = percent_injury_correctly_treated\n",
    "        \n",
    "        # mean_pulse_taken_count\n",
    "        pulse_taken_count = fu.get_pulse_taken_count(scene_df)\n",
    "        row_dict['scene_pulse_taken_count'] = pulse_taken_count\n",
    "        \n",
    "        # mean_stills_value\n",
    "        row_dict['scene_stills_value'] = fu.get_stills_value(scene_df)\n",
    "        \n",
    "        # mean_teleport_count\n",
    "        row_dict['scene_teleport_count'] = fu.get_teleport_count(scene_df)\n",
    "        \n",
    "        # mean_time_to_hemorrhage_control_per_patient\n",
    "        row_dict['scene_time_to_hemorrhage_control_per_patient'] = fu.get_time_to_hemorrhage_control_per_patient(scene_df)\n",
    "        \n",
    "        # mean_triage_time\n",
    "        row_dict['scene_triage_time'] = fu.get_triage_time(scene_df)\n",
    "        \n",
    "        # mean_percent_accurate_tagging\n",
    "        # total_action_count\n",
    "        mask_series = scene_df.action_type.isin(fu.action_types_list)\n",
    "        row_dict['scene_action_count'] = scene_df[mask_series].shape[0]\n",
    "        \n",
    "        # total_assessment_count\n",
    "        mask_series = scene_df.action_type.isin(['PATIENT_ENGAGED', 'PULSE_TAKEN'])\n",
    "        row_dict['scene_assessment_count'] = scene_df[mask_series].shape[0]\n",
    "        \n",
    "        # total_treatment_count\n",
    "        mask_series = scene_df.action_type.isin(['INJURY_TREATED'])\n",
    "        row_dict['scene_treatment_count'] = scene_df[mask_series].shape[0]\n",
    "        \n",
    "        # total_tag_application_count\n",
    "        mask_series = scene_df.action_type.isin(['TAG_APPLIED'])\n",
    "        row_dict['scene_tag_application_count'] = scene_df[mask_series].shape[0]\n",
    "        \n",
    "        # treated_expectant_count\n",
    "    \n",
    "    rows_list.append(row_dict)\n",
    "scene_stats_df = DataFrame(rows_list).drop_duplicates()\n",
    "print(scene_stats_df.shape)\n",
    "display(scene_stats_df.sample(6).T)\n",
    "display(scene_stats_df.groupby('scene_patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b52b98-b35b-4664-91e2-ddafbbab4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the sim environment column\n",
    "new_column_name = 'encounter_layout'\n",
    "encounter_layouts_list = ['Desert', 'Jungle', 'Submarine', 'Urban']\n",
    "for (session_uuid, scene_id), scene_df in csv_stats_df.groupby(fu.scene_groupby_columns):\n",
    "    for env_str in encounter_layouts_list:\n",
    "        patients_list = eval(f'{env_str.lower()}_patients_list')\n",
    "        if all(map(lambda patient_id: patient_id in scene_df.patient_id.unique().tolist(), patients_list)):\n",
    "            mask_series = (scene_stats_df.session_uuid == session_uuid) & (scene_stats_df.scene_id == scene_id)\n",
    "            scene_stats_df.loc[mask_series, new_column_name] = env_str\n",
    "display(scene_stats_df.groupby([new_column_name, 'scene_patient_count'], dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbc10-92a1-45d4-b72f-6dc4cb75d52a",
   "metadata": {},
   "source": [
    "\n",
    "### Add the mean % accurate tagging column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9b418-ea37-4f68-9d76-441fc2d24e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the tag-to-SALT data frame\n",
    "tag_to_salt_df = fu.get_is_tag_correct_data_frame(truncated_df, groupby_column='participant_id')\n",
    "display(tag_to_salt_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3f0f4-d63a-4d05-8fae-c45f2e795fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the percentage tag correct counts for each scene for each group\n",
    "correct_count_by_tag_df = fu.get_percentage_tag_correct_data_frame(tag_to_salt_df, groupby_column='participant_id')\n",
    "display(correct_count_by_tag_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7fbb1-798e-432f-a196-9da17a84fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the scene stats by participant ID to get the tagging accuracy measure\n",
    "for participant_id, idx_df in scene_stats_df.groupby('participant_id'):\n",
    "    \n",
    "    # mean_percent_accurate_tagging\n",
    "    mask_series = (correct_count_by_tag_df.participant_id == participant_id)\n",
    "    scene_stats_df.loc[idx_df.index, 'mean_percent_accurate_tagging'] = correct_count_by_tag_df[mask_series].percentage_tag_correct.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee975601-e5db-44ae-b3a7-1bca9f5bbacd",
   "metadata": {},
   "source": [
    "\n",
    "### Add the treated-expectant count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f70d33-1ec1-4f86-8deb-6e9dba46884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each patient to build the max salt and treated-expectant measures\n",
    "rows_list = []\n",
    "for (session_uuid, scene_id, patient_id), patient_df in truncated_df.groupby(fu.patient_groupby_columns):\n",
    "    row_dict = {cn: eval(cn) for cn in fu.patient_groupby_columns}\n",
    "    row_dict['max_salt'] = fu.get_max_salt(patient_df, session_uuid=session_uuid, scene_id=scene_id, random_patient_id=patient_id)[1]\n",
    "    if (row_dict['max_salt'] == 'EXPECTANT'):\n",
    "        mask_series = ~patient_df.injury_treated_required_procedure.isnull() | ~patient_df.tool_applied_type.isnull()\n",
    "        row_dict['treated_expectant'] = {True: 'yes', False: 'no'}[mask_series.any()]\n",
    "    else: row_dict['treated_expectant'] = nan\n",
    "    rows_list.append(row_dict)\n",
    "treated_expectant_df = DataFrame(rows_list)\n",
    "display(treated_expectant_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea45f11-a4c3-4a21-bc90-1c098af33493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each scene to build the treated-expectant counts\n",
    "for (session_uuid, scene_id), scene_df in treated_expectant_df.groupby(fu.scene_groupby_columns):\n",
    "    mask_series = (scene_df.treated_expectant == 'yes')\n",
    "    \n",
    "    # treated_expectant_count\n",
    "    treated_expectant_count = mask_series.sum()\n",
    "    mask_series = (scene_stats_df.session_uuid == session_uuid) & (scene_stats_df.scene_id == scene_id)\n",
    "    scene_stats_df.loc[mask_series, 'treated_expectant_count'] = treated_expectant_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa2dbe-8ac4-4275-84d5-2d3a343cf5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the survey columns\n",
    "survey_columns = ['AD_KDMA_Sim', 'AD_KDMA_Text', 'PropTrust', 'ST_KDMA_Sim', 'ST_KDMA_Text']\n",
    "if any(map(lambda x: x not in scene_stats_df.columns, survey_columns)):\n",
    "    file_path = osp.join(nu.data_folder, 'xlsx', 'participant_data_0420.xlsx')\n",
    "    participant_data_df = read_excel(file_path).rename(columns={'ParticipantID': 'participant_id'})\n",
    "    print(participant_data_df.shape)\n",
    "    display(participant_data_df.sample(12).T)\n",
    "    \n",
    "    print(\"\\nColumns to merge the participant data with the scene stats on:\")\n",
    "    on_columns = sorted(set(scene_stats_df.columns).intersection(set(participant_data_df.columns)))\n",
    "    assert on_columns, \"You have nothing to merge the participant dataset with the scene stats on\"\n",
    "    print(on_columns)\n",
    "\n",
    "    print(\"\\nThe participant data columns we want to have in the merge:\")\n",
    "    survey_set = set(on_columns + survey_columns)\n",
    "    all_set = set(participant_data_df.columns)\n",
    "    assert survey_set.issubset(all_set), f\"You're missing {survey_set.difference(all_set)} from participant_data_0420.xlsx\"\n",
    "    columns_list = sorted(survey_set)\n",
    "    print(columns_list)\n",
    "    \n",
    "    # mean_AD_KDMA_Sim\n",
    "    # mean_AD_KDMA_Text\n",
    "    # mean_PropTrust\n",
    "    # mean_ST_KDMA_Sim\n",
    "    # mean_ST_KDMA_Text\n",
    "    df = participant_data_df[columns_list]\n",
    "    print(scene_stats_df.shape)\n",
    "    print(df.shape)\n",
    "    scene_stats_df = scene_stats_df.merge(df, how='left', on=on_columns)\n",
    "    print(scene_stats_df.shape)\n",
    "    display(scene_stats_df.groupby(survey_columns, dropna=False).size().to_frame().rename(columns={0: 'record_count'}).sort_values(\n",
    "        'record_count', ascending=False\n",
    "    ).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9592117-1e49-41a4-8d1d-ee0ec74653d9",
   "metadata": {},
   "source": [
    "\n",
    "### Truncate the scene data to only include our patients at the times they were engaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c26ab7-affd-4a41-8a78-072eef3800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out the unnamed layouts\n",
    "mask_series = scene_stats_df.encounter_layout.isin(encounter_layouts_list)\n",
    "pre_count = scene_stats_df.shape[0]\n",
    "scene_stats_df = scene_stats_df[mask_series]\n",
    "print(f\"\\nFiltered out {pre_count - scene_stats_df.shape[0]} unnamed encounter layouts\")\n",
    "display(scene_stats_df.groupby([new_column_name, 'scene_patient_count'], dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77973b18-2f00-4afc-a793-572f4b38dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for zero teleports\n",
    "mask_series = (scene_stats_df.scene_teleport_count < 1)\n",
    "print(f\"\\nThere are {scene_stats_df[mask_series].shape[0]} out of {scene_stats_df.shape[0]} scenes with no teleports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668fd7c-9657-4632-89ca-9c68d9670f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the scene stats dataset\n",
    "nu.save_data_frames(truncated_scene_stats_df=scene_stats_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b780d-76ad-4860-b049-73a06566125a",
   "metadata": {},
   "source": [
    "\n",
    "## Create the ANOVA Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b7629-5f41-4c5a-9ac2-09d6421a1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the columns to merge the scene stats dataset with the CSV stats on\n",
    "print(\"\\nColumns to merge the scene stats dataset with the CSV stats on:\")\n",
    "on_columns = sorted(set(csv_stats_df.columns).intersection(set(scene_stats_df.columns)))\n",
    "print(on_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1840f2c-f18e-4e28-bb2e-fa61a148d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the scene stats dataset columns we want to have in the merge\n",
    "print('\\nThe scene stats dataset columns we want to have in the merge:')\n",
    "mean_analysis_columns = sorted([\n",
    "    'scene_percent_injury_correctly_treated', 'scene_pulse_taken_count', 'scene_stills_value', 'scene_teleport_count',\n",
    "    'scene_time_to_hemorrhage_control_per_patient', 'scene_triage_time', 'scene_patient_count', 'mean_percent_accurate_tagging'\n",
    "] + survey_columns)\n",
    "sum_analysis_columns = sorted([\n",
    "    'scene_action_count', 'scene_assessment_count', 'scene_treatment_count', 'scene_tag_application_count', 'treated_expectant_count'\n",
    "])\n",
    "analysis_set = set(mean_analysis_columns + sum_analysis_columns)\n",
    "all_set = set(scene_stats_df.columns)\n",
    "assert analysis_set.issubset(all_set), f\"You're missing {analysis_set.difference(all_set)} from your analysis_columns\"\n",
    "print(analysis_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0343a-8230-4ae4-96cf-699aa38f79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the scene stats with the CSV stats\n",
    "print(\"\\nMerge the scene stats with the CSV stats\")\n",
    "columns_list = on_columns + mean_analysis_columns + sum_analysis_columns\n",
    "assert set(columns_list).issubset(set(scene_stats_df.columns)), \"You've lost access to the analysis columns\"\n",
    "df = scene_stats_df[columns_list]\n",
    "print(csv_stats_df.shape)\n",
    "print(df.shape)\n",
    "merge_df = csv_stats_df.merge(df, on=on_columns, how='left').drop_duplicates()\n",
    "print(merge_df.shape)\n",
    "display(merge_df.sample(4).T)\n",
    "display(merge_df.groupby('scene_patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd8a41-80c2-4ea6-ae91-b714de276533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the merge dataset columns we want to have in the groupby\n",
    "print('\\nThe merge dataset columns we want to have in the groupby:')\n",
    "columns_list = sorted(set(\n",
    "    on_columns + mean_analysis_columns + sum_analysis_columns\n",
    ").intersection(set(merge_df.columns)))\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d82be-b3e3-4298-bc4f-e35d3fab0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the numeric columns we want to take the mean of\n",
    "print(\"\\nThe numeric columns we want to take the mean of:\")\n",
    "df = merge_df[on_columns + mean_analysis_columns]\n",
    "assert set(df.columns).issubset(set(merge_df.columns)), \"You've lost access to the mean analysis columns\"\n",
    "mean_numeric_columns = sorted(set(nu.get_numeric_columns(df)).difference(set(\n",
    "    on_columns\n",
    ")))\n",
    "print(mean_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8ff21-c9ab-4786-bc06-d481ccecaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the numeric columns we want to take the sum of\n",
    "print(\"\\nThe numeric columns we want to take the sum of:\")\n",
    "df = merge_df[on_columns + sum_analysis_columns]\n",
    "assert set(df.columns).issubset(set(merge_df.columns)), \"You've lost access to the sum analysis columns\"\n",
    "sum_numeric_columns = sorted(set(nu.get_numeric_columns(df)).difference(set(\n",
    "    on_columns\n",
    ")))\n",
    "print(sum_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279636b-907a-4e45-98f7-9eca3032665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the other columns we do not want to take the mean or sum of\n",
    "print(\"\\nThe other columns we do not want to take the mean or sum of:\")\n",
    "other_columns = sorted(set(on_columns + mean_analysis_columns + sum_analysis_columns).difference(set(\n",
    "    mean_numeric_columns + sum_numeric_columns\n",
    ")))\n",
    "print(other_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f216fca-b55a-4ae6-a7c3-47eb15561d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the means and sums datasets and the columns to merge the summed half of the merge with the meaned half of the merge on\n",
    "means_df = merge_df[mean_numeric_columns+on_columns].groupby(on_columns).mean().reset_index(drop=False).rename(\n",
    "    columns={cn: 'mean_'+cn.replace('mean_', '').replace('scene_', '') for cn in mean_numeric_columns}\n",
    ").dropna(axis='columns', how='all')\n",
    "# display(means_df.sample(7).T)\n",
    "# display(means_df.groupby('mean_patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))\n",
    "sums_df = merge_df[sum_numeric_columns+on_columns].groupby(on_columns).sum().reset_index(drop=False).rename(\n",
    "    columns={cn: 'sum_'+cn.replace('sum_', '').replace('scene_', '') for cn in sum_numeric_columns}\n",
    ").dropna(axis='columns', how='all')\n",
    "# display(sums_df.sample(7).T)\n",
    "print(\"\\nColumns to merge the summed half of the merge with the meaned half of the merge on:\")\n",
    "on_columns = sorted(set(means_df.columns).intersection(set(sums_df.columns)))\n",
    "print(on_columns)\n",
    "print(means_df.shape)\n",
    "print(sums_df.shape)\n",
    "left_df = means_df.merge(sums_df, on=on_columns, how='outer').drop_duplicates()\n",
    "print(left_df.shape)\n",
    "# display(left_df.sample(7).T)\n",
    "# display(left_df.groupby('mean_patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83901b9-31fa-4525-8d28-ba7480bac6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the columns to merge the unaggregated half of the merge with the aggregated half of the merge on\n",
    "right_df = merge_df[other_columns].drop_duplicates().dropna(axis='columns', how='all')\n",
    "print(right_df.shape)\n",
    "display(right_df.sample(5))\n",
    "print(\"\\nColumns to merge the unaggregated half of the merge with the aggregated half of the merge on:\")\n",
    "on_columns = sorted(set(left_df.columns).intersection(set(right_df.columns)))\n",
    "print(on_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ef330-a69c-42f9-a525-cc862204591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the unaggregated half of the merge with the aggregated half\n",
    "print(\n",
    "    \"\\nAggregate the data from the merged datasets and group by participant, session,\"\n",
    "    \" and scene to get the means and sums of the numeric columns\"\n",
    ")\n",
    "print(left_df.shape)\n",
    "print(right_df.shape)\n",
    "anova_df = left_df.merge(right_df, on=on_columns, how='outer').drop_duplicates()\n",
    "print(anova_df.shape)\n",
    "display(anova_df.sample(7).T)\n",
    "display(anova_df.groupby('mean_patient_count', dropna=False).size().to_frame().rename(columns={0: 'record_count'}))\n",
    "assert set(\n",
    "    ['mean_'+cn for cn in survey_columns]\n",
    ").issubset(set(anova_df.columns)), \"You've lost acces to the survey columns (PropTrust, et al)\"\n",
    "assert len(anova_df.groupby(\n",
    "    ['participant_id', 'scene_id', 'session_uuid']\n",
    ").groups.keys()) == anova_df.shape[0], \"You have duplicate rows in anova_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5ea69-2034-428e-b9fe-ece3f7a920e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the sim environment back in\n",
    "new_column_name = 'encounter_layout'\n",
    "if new_column_name not in anova_df.columns:\n",
    "    print(\"\\nAdd the sim environment back in\")\n",
    "    on_columns = sorted(set(anova_df.columns).intersection(set(scene_stats_df.columns)))\n",
    "    columns_list = on_columns + [new_column_name]\n",
    "    assert set(columns_list).issubset(set(scene_stats_df.columns)), f\"You've lost acces to the {new_column_name} column\"\n",
    "    df = scene_stats_df[columns_list]\n",
    "    print(anova_df.shape)\n",
    "    print(df.shape)\n",
    "    anova_df = anova_df.merge(\n",
    "        df, on=on_columns, how='left'\n",
    "    )\n",
    "    print(anova_df.shape)\n",
    "    display(anova_df.groupby([new_column_name, 'mean_patient_count'], dropna=False).size().to_frame().rename(\n",
    "        columns={0: 'record_count'}\n",
    "    ).sort_values('record_count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d5280-5a4f-4593-8b74-b1cf22429fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out the unnamed layouts\n",
    "mask_series = anova_df.encounter_layout.isin(encounter_layouts_list)\n",
    "pre_count = anova_df.shape[0]\n",
    "anova_df = anova_df[mask_series]\n",
    "print(f\"\\nFiltered out {pre_count - anova_df.shape[0]} unnamed encounter layouts\")\n",
    "display(anova_df.groupby(['encounter_layout', 'mean_patient_count'], dropna=False).size().to_frame().rename(\n",
    "    columns={0: 'record_count'}\n",
    ").sort_values('record_count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7962f0-138e-4da1-b277-67d4e7145bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for low patient counts\n",
    "mask_series = (anova_df.mean_patient_count < 4)\n",
    "print(f\"\\nThere are {anova_df[mask_series].shape[0]} out of {anova_df.shape[0]} participations with low patient counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d400abb-f011-41b2-9b30-9db51904faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store the results\n",
    "columns_list = anova_df.columns.tolist()\n",
    "nu.save_data_frames(truncated_anova_df=anova_df[columns_list], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbffa6f-c542-480e-aa38-8573e1dc0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get statistics using OSU format\n",
    "columns_list = ['mean_'+cn for cn in survey_columns] + [\n",
    "    'mean_percent_accurate_tagging', 'mean_patient_count', 'mean_percent_injury_correctly_treated', 'mean_pulse_taken_count',\n",
    "    'mean_stills_value', 'mean_teleport_count', 'mean_time_to_hemorrhage_control_per_patient', 'mean_triage_time',\n",
    "    'sum_action_count', 'sum_assessment_count', 'sum_tag_application_count', 'sum_treatment_count', 'sum_treated_expectant_count'\n",
    "]\n",
    "description_df = nu.get_statistics(anova_df, columns_list).T\n",
    "assert float(description_df.loc['mean_patient_count', 'min']) >= 4.0, \"There are not less than 4 patients in any scene\"\n",
    "\n",
    "# Calculate range and IQR\n",
    "description_df['range'] = description_df['max'] - description_df['min']\n",
    "description_df['IQR'] = description_df['75%'] - description_df['25%']\n",
    "\n",
    "# Define the index list\n",
    "index_list = ['min', 'median', 'max', 'IQR', 'range', 'mean', 'SD']\n",
    "\n",
    "# Set formatting to prevent scientific notation (assuming numeric columns)\n",
    "description_df = description_df[index_list].applymap('{:.4f}'.format)  # Format as floats with 4 decimals\n",
    "\n",
    "# Save and show the description data frame\n",
    "nu.save_data_frames(description_df=description_df, verbose=True)\n",
    "display(description_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c712f7a-2295-48ce-8841-d178d07f0949",
   "metadata": {},
   "source": [
    "\n",
    "# Plot the Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f747228-6c18-4049-b048-6ae2f64dff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a means to plot a correlation graph\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_graph(correlations, title_str='Correlation Matrix of ??', fig=None, ax=None):\n",
    "    \n",
    "    # Plotting the correlation matrix using matplotlib\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    norm = colors.Normalize(vmin=-1, vmax=1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    if (fig is None) or (ax is None):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(correlations, cmap=cmap, norm=norm)\n",
    "\n",
    "    # Add a color bar\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(len(correlations.columns)))\n",
    "    ax.set_yticks(np.arange(len(correlations.columns)))\n",
    "    ax.set_xticklabels(correlations.columns)\n",
    "    ax.set_yticklabels(correlations.columns)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='left', rotation_mode='anchor')\n",
    "\n",
    "    # Add text annotations.\n",
    "    for (i, j), val in np.ndenumerate(correlations):\n",
    "        ax.text(j, i, f'{val:.3f}', ha='center', va='center', color='black', fontsize=84/correlations.shape[0])\n",
    "    plt.title(title_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de6a2e-9fe8-4e26-83d0-b12853ec5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the correlations of all the numeric columns in the ANOVA dataset\n",
    "columns_list = ['mean_'+cn for cn in survey_columns] + [\n",
    "    'mean_percent_accurate_tagging', 'mean_patient_count', 'mean_percent_injury_correctly_treated', 'mean_pulse_taken_count',\n",
    "    'mean_stills_value', 'mean_teleport_count', 'mean_time_to_hemorrhage_control_per_patient', 'mean_triage_time',\n",
    "    'sum_action_count', 'sum_assessment_count', 'sum_tag_application_count', 'sum_treatment_count', 'sum_treated_expectant_count'\n",
    "]\n",
    "# columns_list = ['mean_'+cn for cn in survey_columns]\n",
    "df = anova_df[columns_list]\n",
    "correlations_df = df.corr().round(3)\n",
    "display(correlations_df.sample(4).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b69682-8c51-4d73-9c60-dcc86f09d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "nu.save_data_frames(correlations_df=correlations_df, verbose=True)\n",
    "fig_height = 7\n",
    "fig_size=(nu.twitter_aspect_ratio*fig_height, fig_height)\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "title_str = \"Correlation Matrix of 18 Analytic and KDMA Columns\"\n",
    "plot_correlation_graph(correlations_df, title_str=title_str, fig=fig, ax=ax)\n",
    "dir_names_list=['png', 'svg']\n",
    "for dir_name in dir_names_list:\n",
    "    try:\n",
    "        dir_path = osp.join(nu.saves_folder, dir_name)\n",
    "        os.makedirs(name=dir_path, exist_ok=True)\n",
    "        file_path = osp.join(dir_path, '{}.{}'.format(re.sub('[^A-Za-z0-9]+', '_', title_str).lower(), dir_name))\n",
    "        if osp.exists(file_path): os.remove(file_path)\n",
    "        print(f'Saving to {osp.abspath(file_path)}')\n",
    "        fig.savefig(file_path, bbox_inches='tight')\n",
    "    except Exception as e:\n",
    "        print(print(f'{e.__class__} error saving the figure as a {dir_name}: {str(e).strip()}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8aea88-036f-47ff-bf3b-8536fc82dfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITM Analysis Reporting (Python 3.11.7)",
   "language": "python",
   "name": "itm_analysis_reporting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
